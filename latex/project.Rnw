% Created: Tue Feb 17 15:26:53 CET 2015
\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc} % Norwegian letters
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{float}  % Used for minipage and stuff.
\usepackage{wrapfig} % Wrap text around figure wrapfig [tab]
\usepackage{graphicx}
\usepackage{enumerate} % Use e.g. \begin{enumerate}[a)]
\usepackage[font={small, it}]{caption} % captions on figures and tables
\usepackage[toc,page]{appendix} % Page make Appendices title, toc fix table of content 
\usepackage{todonotes} % Notes. Use \todo{"text"}. Comment out \listoftodos
\usepackage{microtype} % Improves spacing. Include AFTER fonts
\usepackage{hyperref} % Use \autoref{} and \nameref{}
\hypersetup{backref,
  colorlinks=true,
  breaklinks=true,
  %hidelinks, %uncomment to make links black
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}
\usepackage[all]{hypcap} % Makes hyperref jup to top of pictures and tables
%
%-------------------------------------------------------------------------------
% Page layout
%\usepackage{showframe} % Uncomment if you want the margin frames
\usepackage{fullpage}
\topmargin=-0.25in
%\evensidemargin=-0.3in
%\oddsidemargin=-0.3in
%\textwidth=6.9in
%\textheight=9.5in
\headsep=0.25in
\footskip=0.50in

%-------------------------------------------------------------------------------
% Header and footer
\usepackage{lastpage} % To be able to add last page in footer.
\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancy} % Use "fancyplain" for header in all pages
%\renewcommand{\chaptermark}[1]{ \markboth{#1}{} } % Usefull for book?
\renewcommand{\sectionmark}[1]{ \markright{\thesection\ #1}{} } % Remove formating and nr.
%\fancyhead[LE, RO]{\footnotesize\leftmark}
%\fancyhead[RO, LE]{\footnotesize\rightmark}
\lhead[]{\AuthorName}
\rhead[]{\rightmark}
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{Page\ \thepage\ of\ \protect\pageref*{LastPage}} % Page numbering for right footer
\renewcommand{\headrulewidth}{1pt} % header underlines
\renewcommand{\footrulewidth}{1pt} % footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

%-------------------------------------------------------------------------------
% Suppose to make it easier for LaTeX to place figures and tables where I want.
\setlength{\abovecaptionskip}{0pt plus 1pt minus 2pt} % Makes caption come closer to figure.
%\setcounter{totalnumber}{5}
%\renewcommand{\textfraction}{0.05}
%\renewcommand{\topfraction}{0.95}
%\renewcommand{\bottomfraction}{0.95}
%\renewcommand{\floatpagefraction}{0.35}
%
% Math short cuts for expectation, variance and covariance
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
% Commands for argmin and argmax
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-----------------------------------------------------------------------------
%	TITLE SECTION
%-----------------------------------------------------------------------------
\newcommand{\AuthorName}{HÃ¥vard Kvamme} % Your name

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{NTNU} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Project Classification \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{\AuthorName } % Your name

\date{\normalsize\today} % Today's date or a custom date
\begin{document}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listoftodos{}
%
%
\section{Introduction}
\label{sec:Introduction}
\todo{Past tense?}
This project is an introduction to classification in statistics. The goal was get an overview of methods used in classification and further use this knowledge in a master thesis. \\
\\
The idea in classification is to find to which of a set of categories a new observation belongs. This is done by creating a classifier on a training set of observed data $(\mathbf{x}_i, y_i)$, for $i = 1, \ldots, N$, where $\mathbf{x}_i$ is a vector of features, and $y_i$ is to which class it belongs. Over time, there are a variety of approaches, no one methods that can claim the title of the best. This is because different assumptions fits different datasets, and the size of the training data is also important to consider. \\
\colorbox{yellow}{Write something about how bad methods with larger training sets do better than good algorithms with smaller sets}\\
\colorbox{yellow}{Should do some simulations on this.}


\subsection{Notation}
\label{sub:Notation}

\section{Linear Classifiers}
\label{sec:Linear Classifiers}
It is reasonable to start with two of the most intuitive and famous methods for classification, LDA and Logistic Regression. The point of the following chapters is to give a general introduction to some methods for classification, and therefore only two classes will be considered. Later this framework will be expanded to include multiple classes.
%
\subsection{LDA and Fisher's linear discriminant}
\label{sub:LDA and Fisher's linear discriminant}
This section should be considered a summary from \cite[ch.~4.1.4]{bishop} and \cite[ch.~4.3]{modstat}, and no further references to these books will be made. \\
\\
The idea in Fisher's linear discriminant is to find a discriminant hyperplane in the features $\mathbf{x}\in \mathbb{R}^D$. This is done by finding a vector $\mathbf{w}$, that the features are projected on and thus reducing the problem to one dimension in 
\begin{align}
  z = \mathbf{w}'\mathbf{x}.
\end{align}
If $\mathbf{w}$ is chosen well, one can find a threshold $t$, and classify to class $\mathcal{C}_1$ if $z > t$ and to class $\mathcal{C}_2$ if not. The problem is therefore, how to find the vector $\mathbf{w}$ that gives the best class separation. One approach is to let $\mathbf{w}$ be proportional to the vector between the class means, i.e. $\mathbf{w} \propto (\bm{\mu}_2 - \bm{\mu}_1)$, where $\bm{\mu}_k = \sum_{i \in \mathcal{C}_k} \mathbf{x}_i/N_k$, and $N_k$ is the number of training points in $\mathcal{C}_k$. However, this does not take into account the orientation of the points around the mean. This causes problems if the variance is high in some direction perpendicular to $(\bm{\mu}_2 - \bm{\mu}_1)$. Fisher's approach is to also take this within-class variance into account, by minimize it on the projected line. So he want the $\mathbf{w}$ that maximize the between-class variance $(\mu_2 - \mu_1)^2$ and minimize the within-class variance $s_1^2 + s_2^2$. 
\begin{align}
  \label{eq:fisherMax} 
  \mathbf{w} &= \argmax_{\mathbf{v}} \frac{(\mu_2 - \mu_1)^2}{s_1^2 + s_2^2} \\
  s_k^2 &= \sum_{i \in \mathcal{C}_k} (z_i - m_k)^2, \quad \mu_k = \mathbf{v}'\bm{\mu}_k.
\end{align}
Now let 
\begin{align}
  \mathbf{S}_W &= \mathbf{S}_1 + \mathbf{S}_2, \quad  \mathbf{S}_k =  \sum_{i \in \mathcal{C}_k} (\mathbf{x}_i - \bm{\mu}_k)(\mathbf{x}_i - \bm{\mu}_k)', \\
  \label{eq:SB} 
  \mathbf{S}_B &= (\bm{\mu}_2 - \bm{\mu}_1)(\bm{\mu}_2 - \bm{\mu}_1)'
\end{align}
\colorbox{yellow}{1/(N-1) differs in wikipedia and bishop}\\
denote the total within- and between-class covariances, and \eqref{eq:fisherMax} can be written as
\\ \colorbox{yellow}{Not 1/N means we weight covariances by nr of points}\\
\begin{align}
  \mathbf{w} = \argmax_v  \frac{\mathbf{v}'\mathbf{S}_B \mathbf{v}}{\mathbf{v}' \mathbf{S}_W \mathbf{v}}.
\end{align}
By differentiating and setting the expression equal to zero one get
\begin{align}
  (\mathbf{w}' \mathbf{S}_B \mathbf{w})\mathbf{S}_W \mathbf{w} = (\mathbf{w}' \mathbf{S}_W \mathbf{w})\mathbf{S}_B \mathbf{w}.
\end{align}
From \eqref{eq:SB}, it follows that that $(\bm{\mu}_2 - \bm{\mu}_1)$ is proportional to $\mathbf{S}_B \mathbf{v}$. This yields Fisher's linear discriminant
\begin{align}
  \mathbf{w} \propto \mathbf{S}_W^{-1} (\bm \mu_2 - \bm \mu_1).
\end{align}
Note that even the terms \textit{variance} and \textit{covariance} are used, the expressions are not averaged. This is just to simplify notation. \\
\\
The term Linear discriminant analysis, or LDA, and Fisher's linear discriminant are often used interchangeably. Only the derivation differs. For LDA it takes a Bayesian approach. Assume the prior distribution of the classes $Y=k$, $\pi_k$, and the conditional distribution of $X$ given $Y$, $f_k(\mathbf{x})$ are known. Simple application of the Bayes theorem yields 
\begin{align}
  P(Y=k|X=\mathbf{x}) = \frac{f_k(\mathbf{x}) \pi_k}{\sum^{K}_{i=1} f_i(\mathbf{x})\pi_i} .
\end{align}
Using this posterior distribution to create
\begin{align}
  \label{eq:bayesian} 
  C(\mathbf{x}) = \argmax_k P(Y=k|X=\mathbf{x}),
\end{align}
which is called the \textit{Bayes classifier} \cite[p.~21]{modstat}. It is clear that this is the best one can do, and the Bayes classifier is therefor often used as a benchmark in simulation studies. As the prior and conditional distribution are not know, there are numerous approaches use different assumptions on these. LDA assume the conditional distributions are multivariate Gaussian with same covariance $\bm{\Sigma}$ and different mean $\bm\mu_k$
\begin{align}
  f_k(\mathbf{x}) =  \frac{1}{(2\pi)^{N_k/2}|\bm{\Sigma}|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{x}-\bm \mu_k)' \bm{\Sigma}^{-1} (\mathbf{x}-\bm \mu_k)\right).
\end{align}
The classifier is constructed by evaluating the log-ratio of the probabilities
\begin{align}
  \label{eq:lda} 
  \log{\frac{P(Y=2|X=\mathbf{x})}{P(Y=1|X=\mathbf{x})}} &= \log{\frac{f_2(\mathbf{x})}{f_1(\mathbf{x})} } + \log{\frac{\pi_2}{\pi_1} } \notag \ \\
  &= \log{\frac{\pi_2}{\pi_1}} -\frac{1}{2} (\bm \mu_2+\bm \mu_1)' \bm{\Sigma}^{-1} (\bm \mu_2-\bm \mu_1) \\
  & \quad + \mathbf{x}'\bm{\Sigma}^{-1} (\bm \mu_2-\bm \mu_1).
  \notag \ 
\end{align}
So one classify to $\mathcal{C}_2$ if \eqref{eq:lda} is positive and to $\mathcal{C}_1$ if not. Another approach is to use the only last part of \eqref{eq:lda}, $\mathbf{x}'\bm{\Sigma}^{-1} (\bm \mu_1-\bm \mu_2)$, and classify based on some threshold $t$ found by e.g. cross-validation. The reasoning behind this is that if the Gaussian assumptions are wrong, a better threshold can be created based on the training data. \\
\\
When using LDA, the parameters are not know, so they are usually estimated from the training data by
\begin{align}
   \hat{\pi}_k &= \frac{N_k}{N},  \\
   \hat{\bm \mu}_k &= \frac{1}{N_k} \sum_{i \in \mathcal{C}_k}\mathbf{x}_i, \\
   \hat{\bm \Sigma} &= \frac{1}{N-2} \sum_{k = 1}^{2} \sum_{i \in \mathcal{C}_k}(\mathbf{x}_i - \hat{\bm{\mu}}_k)(\mathbf{x}_i - \hat{\bm{\mu}}_k)'.
\end{align}
It now becomes clear that both LDA and Fisher's linear discriminant do the same projection. The only difference between the methods is that because of the Gaussian assumptions LDA can give a suggested threshold.
%
\subsection{Logistic Regression}
\label{sub:Logistic Regression}
\colorbox{yellow}{Optimization algorithms for finding parameters?}\\
The goal of Logistic regression is to model the posterior class-probabilities and create a classifier based on them. As the probabilities should sum to one, it is obvious that they can not be linear in $\mathbf{x}$. The approach is therefore to assume the log-odds are linear in $\mathbf{x}$. 
\begin{align}
  \label{eq:logclass} 
   \log \frac{P(Y=2|X=\mathbf{x})}{P(Y=1|X=\mathbf{x})} = \beta_0 + \mathbf{x}'\bm \beta.
\end{align}
From now on it is assumed that $\beta_0 + \mathbf{x}' \bm \beta$ is written as $\mathbf{x}' \bm \beta$.
The function
\begin{align}
  logit(p_{ki}) = \log  \frac{p_{ki}}{1-p_{ki}},
\end{align}
where $p_{ki} = P(Y=k|X=\mathbf{x_i})$, is called \textit{the logit function} or \textit{link}. By inverting the logit function it becomes clear that the posterior probabilities sum to \textit{one}.
\begin{align}
  p_{2i} &=  \frac{e^{\mathbf{x}_i'\bm \beta}}{1 + e^{\mathbf{x}_i'\bm \beta}} \\
  p_{1i} &=  \frac{1}{1 + e^{\mathbf{x}_i'\bm \beta}}.
\end{align}
There are other choices than the logit function, e.g. the probit function based on Gaussian assumptions, however, they will not be covered in this paper. 
\todo{Refer to paper?}
Often there is little knowledge about the posterior distribution, so the logit might be as good a choice as any other. 
\todo{Remove this?}
To find good values for $\bm \beta$, it is common to use the MLE (Maximum likelihood estimator). Let $n_i$ be the number of training samples in gourp $i$ (common $\mathbf{x}_i$), and let $Z_i$ the amount in group $i$ that has class $2$. This means $Z_i$ is binomial distributed with
\begin{align}
  P(Z_i = z_i) = \binom{n_i}{z_i} p_{2i}^{z_i} (1-p_{2i})^{n_i - z_i}.
\end{align}
So the likelihood and log-likelihood are
\begin{align}
  L(\beta) &= \prod_{i = 1}^{N} P(Z_i = z_i). \\
  l(\beta) &\propto \sum^{N}_{i=1} z_i \log p_{2i} + (n_i - z_i) \log (1- p_{2i}).
\end{align}
There is no analytical solution that maximizes the likelihood, so one has to use a numerical optimization algorithm to find the MLE.  \todo{Elaborate optimization?}
After $\bm \beta$ is found, one can create a classifier the same way as for logistic regression, i.e. classify to $\mathcal{C}_2$ if \eqref{eq:logclass} is positive and to $\mathcal{C}_1$ if not. However, this assumes the logit link gives accurate probabilities. Often the choice of logit link is based on lack of a better choice so there is little suggesting the probabilities are particularly accurate. It might therefore be better to classify based on a threshold found by e.g. cross-validation. 
\\\colorbox{yellow}{Have hear a lot that logit is used because: "Why not".}
%
\subsection{LDA and Logistic Regression}
\label{sub:LDA and Logistic Regre}
<<echo = FALSE>>=
source("../code/ldaVsLogistic.R")
@
\colorbox{yellow}{Show an example of how Logistic compares to LDA with and without thresholds.}\\
Comparing LDA and Logistic regression, it is clear that they are both linear in the log odds. Obviously, this does not restrict the decision boundary to be linear in $x_1, x_2, \ldots , x_D$. One can easily let $\mathbf{x}$ contain polynomial terms and interaction terms, like $(x_1, x_2, x_1^2, x_1 x_2, \ldots )$. This way one see that the decision boundary can be pretty flexible. Both methods are also easy to generalize to multi class classifiers, but this will not be done in this section.
\colorbox{yellow}{Do it later?}
Hello


\begin{figure}[h!]
\begin{center}
    \includegraphics[scale=0.5]{./figures/ldaVsLogistic.pdf}
\end{center}
\caption{Look at this}
\label{fig:name}
\end{figure}




%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\clearpage
%\begin{appendices}
  %% Use \section
  %% Can use \phantomsection in text to get back
%\end{appendices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\begin{thebibliography}{99} % At most 99 references.
	\bibitem{bishop}
  Christopher M. Bishop,
  \emph{Pattern recognition and machine learning}.
	\bibitem{modstat}
  Trevor Hastie, Robert Tibshirani, Jerome Friedman,
  \emph{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}.
% Use "bibit" to generate bibitem
\end{thebibliography}
%
\end{document}
