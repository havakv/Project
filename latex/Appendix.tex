\chapter{Methods for validation}
\label{chap:Metods for validation}
\todo{Find better name}
\section{EPE}
\label{sec:EPE}
The engine in prediction problems is to minimize the expected prediction error $\mathrm{EPE}$ of some function $\hat f$, under some loss function $L$
\\\colorbox{yellow}{Find good notation $\hat f$ when comparing with sections.}
\begin{align}
  \mathrm{EPE}(\hat f) = \E_{X Y}[L(Y, \hat f(X))] = \E_X [\E_Y (L(Y, \hat f(X))|X)].
\end{align}
Thus 
\begin{align}
  f = \argmin_{\hat f} \mathrm{EPE}(\hat f) = \argmin_{\hat f} \E_Y [L(Y, \hat f(X)) | X ].
\end{align}
Under squared error loss, $L(Y, \hat f(X)) = (Y- \hat f(X)^2)$, the solution is 
$ f(x) = \E[Y | X=x]$. And under $0/1$ loss $ f(x)$ is the \textit{Bayes classifier} 
\begin{align}
  f(x) = \E[Y | X=x].
\end{align}

\subsection{Bias-variance tradeoff}
\label{sub:Bias-variance tradeoff}
In the regression framework the relation,
\begin{align}
  Y = f(X) + \varepsilon,  
\end{align}
is assumed, where $\varepsilon$ has zero mean and variance $\sigma^2$.
Under this assumption $\mathrm{EPE}$ can be decomposed into
\begin{align}
  \mathrm{EPE}(\hat f) &=  \E_Y[Y^2] + \E_X[X^2] - \E_{X Y}[2 y \hat f(X)] \notag \\
                       &= \Var[Y] + \E_Y[Y]^2 + \Var[X] + \E_X[X]^2 - 2 f\: \E_X[\hat f(X)] \notag \\
                       &= \Var[Y] + \Var[\hat f(X)] + (f - \E_X[\hat f(X)])^2 \notag \\
                       &= \sigma^2 + \Var[\hat f(X)] + \mathrm{Bias}[\hat f(X)]^2.
\end{align}
So to minimize $\mathrm{EPE}(\hat f)$ it is necessary to minimize the variance and bias of $\hat f$. This equation is often referred to as the bias-variance tradeoff as usually when one is decreased the other increase. 

As discussed in \ref{sec:Tree}, there is no equally satisfactory decomposition of $\mathrm{EPE}$ under $0/1$ loss, but it is still common to talk about the bias-variance tradeoff in classification as well.



\section{Cross-validation}
\label{sec:Cross-validation}
Ideally, when one have large amount of data, a validation set is set a side to assess the performance of the classifier in question (can also be used for regression). This is useful when tuning of the model is required for good results. When data is scarce, other methods for validation has to be used instead. Cross-validation is probably the simples and most widely used estimate for the test error \citep{modstat}, and there are different version of it. In this project only \textit{K-fold} cross-validation is discussed. 

The idea is simple. Create a random permutation of the dataset and divide it into $K$ subsets. Then remove the first subset and train the algorithm on the remaining $K-1$. The first subset can now be used as a test set. Next, remove the second subset, train, and test. When this is done for all the $K$ sets, the average prediction error is a fairly good estimate of the actual test error. 

The choice of $K$ is important to consider. The number of fittings required is equal to $K$, so a small $K$ gives an computational advantage. So for $N$ data points, a choice of $K = N$, also called \textit{leave-one-out} cross-validation, is the most computationally expensive. However, as almost all data points are used to train on, it is approximately unbiased. On the other hand, as the $N$ different fittings are highly correlated, the variance of leave-one-out is quite high. If $K$ is chosen small, it will decrease the variance but increase the bias. So the best value for $K$ is dependent on the problem. Two particularly common choices for $K$ are 10 and 5.


\section{Bootstrapping}
\label{sec:Bootstrapping}
Bootstrapping refers to methods based on random sampling with replacement. It is used to approximate the distribution of data, based on the data itself. One can thus do inference on data, like for instance a confidence interval for a parameter estimate. It can also be used to improve predictions, like in the Bagging algorithm in Section~\ref{sec:Bagging}. In the context of this project, bootstrapping refers to sample $N$ data points from the dataset $\left\{ \mathbf{x}_i, y_i \right\}_{i = 1}^N$, with replacement, and is only used for improving predictions. 
For an introduction to bootstrapping, see \cite{efron1994bootstrap}.


\section{Variance for Random Forests regression}
\label{sec:Variance for Random Forests regression}
Let $T_i(\mathbf{x})$ denote a trained tree. The Random Forests prediction for $\mathbf{x}$ is 
\begin{align}
  \hat f(\mathbf{x}) = \frac{1}{B} \sum_{i = 1}^{B} T_i(\mathbf{x}).
\end{align}
The trees are created by drawing from the same distributions, so they are identically distributed, with covariance,
\begin{align}
  \Cov[T_i(\mathbf{x}), T_j(\mathbf{x}))] = \rho \sigma^2, \quad \quad i \neq j.
\end{align}
Here $\sigma^2$ is the variance of a tree and $\rho$ is the correlation between two trees.  
The variance of $\hat f(\mathbf{x})$ is thus,
\begin{align}
\Var\left[ \frac{1}{B} \sum_{i = 1}^{B} T_i(\mathbf{x}) \right] 
&= \frac{1}{B^2} \sum_{i =1}^B \sum_{j= 1}^B \Cov[T_i(\mathbf{x}), T_j(\mathbf{x})] \notag \\
&= \frac{1}{B^2} \sum_{i =1}^B \left(\sum_{j \neq i} \Cov[T_i(\mathbf{x}), T_j(\mathbf{x})] + \Var[T_i(\mathbf{x})]  \right)\notag \\
&= \frac{1}{B^2} \sum_{i =1}^B \left( (B-1) \rho \sigma^2 + \sigma^2  \right)\notag \\
&= \rho \sigma^2 + \sigma^2 \frac{1-\rho}{B}.
\end{align}
So the variance decrease by increasing the number of bootstrap samples $B$ and by decorrelating the trees.

\chapter{Code from experiments}
\label{chap:Code}
Make sure to refer to this appendix in the experiments chapter.
