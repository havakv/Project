\chapter{Methods for validation}
\label{chap:Metods for validation}
\section{EPE}
\label{sec:EPE}
The goal of the prediction problems in this report is to minimize the expected prediction error $\mathrm{EPE}$ of some function $f$, under some loss function $L$,
\begin{align}
  \mathrm{EPE}(f) = \E_{\mathbf{x} y}[L(y, f(\mathbf{x}))] = \E_{\mathbf{x}} [\E_y (L(y, f(\mathbf{x})) \mid \mathbf{x})].
\end{align}
Thus,
\begin{align}
  f = \argmin_{f} \mathrm{EPE}(f) = \argmin_{ f} \E_y [L(y, f(\mathbf{x})) \mid \mathbf{x}\: ].
\end{align}
Under squared error loss, $L(y, f(\mathbf{x})) = (y- f(\mathbf{x}))^2$, the solution is 
$ f(\mathbf{x}) = \E[\:y \mid \mathbf{x}\:]$. And under $0/1$ loss $ f(\mathbf{x})$ is the \textit{Bayes classifier},
\begin{align}
  f(\mathbf{x}) = \E[\: y \mid \mathbf{x} \:].
\end{align}

\subsection{Bias-variance tradeoff}
\label{sub:Bias-variance tradeoff}
In the regression framework the relation,
\begin{align}
  y = g(X) + \varepsilon,  
\end{align}
is assumed, where $\varepsilon$ has zero mean and variance $\sigma^2$. Let $f$ be an estimator of $g$.
Under these assumptions the $\mathrm{EPE}$ of $f$ under squared error loss can be decomposed into,
\begin{align}
  \mathrm{EPE}(f) &=  \E_y[y^2] + \E_\mathbf{x}[\mathbf{x}^2] - \E_{\mathbf{x} y}[2 y f(\mathbf{x})] \notag \\
                       &= \Var[y] + \E_y[y]^2 + \Var[\mathbf{x}] + \E_\mathbf{x}[\mathbf{x}]^2 - 2 g\: \E_\mathbf{x}[f(\mathbf{x})] \notag \\
                       &= \Var[y] + \Var[ f(\mathbf{x})] + (g - \E_\mathbf{x}[ f(\mathbf{x})])^2 \notag \\
                       &= \sigma^2 + \Var[ f(\mathbf{x})] + \mathrm{Bias}[f(\mathbf{x})]^2.
\end{align}
So to minimize $\mathrm{EPE}(f)$ it is necessary to minimize the variance and bias of $f$. This equation is often referred in combination with bias-variance tradeoff. That comes from the face that when the variance decreased, usually the bias increase, and vise versa. So to minimize the $\mathrm{EPE}$ the tradeoff has to be balanced. 

As discussed in \ref{sec:Tree}, there is no equally satisfactory decomposition of $\mathrm{EPE}$ under $0/1$ loss, but it is still common to talk about the bias-variance tradeoff in classification as well.



\section{Cross-validation}
\label{sec:Cross-validation}
When given a large dataset, a validation set can be set a side to assess the performance of the classifier in question (can also be used for regression). This is useful when tuning of the model is required for good results. When data is scarce, other methods for validation has to be used instead. Cross-validation is probably the simples and most widely used estimate for the test error \citep{modstat}, and there are different version of it. In this project only \textit{K-fold} cross-validation is discussed. 

The idea is simple. Create a random permutation of the dataset and divide it into $K$ subsets. Then remove the first subset and train the algorithm on the remaining $K-1$. The first subset can now be used as a test set. Next, remove the second subset, train on the other $K-1$, and test. When this is done for all the $K$ sets, the average prediction error is a fairly good estimate of the actual test error. 

The choice of $K$ is important to consider. The number of fittings required is equal to $K$, so a small $K$ gives a computational advantage. So for $N$ data points, a choice of $K = N$, also called \textit{leave-one-out} cross-validation, is the most computationally expensive. However, as almost all data points are used to train on, it is approximately unbiased. On the other hand, as the $N$ different fittings are highly correlated, the variance of leave-one-out is quite high. If $K$ is small, it will decrease the variance but increase the bias. So the best value for $K$ is dependent on the problem. Two particularly common choices for $K$ are 10 and 5.


\section{Bootstrapping}
\label{sec:Bootstrapping}
Bootstrapping refers to methods based on random sampling with replacement. It is used to approximate the distribution of the data. It enables new possibilities to do inference on data, like for instance a confidence interval for a parameter estimate without any assumptions on the distribution. It can also be used to improve predictions, like in the Bagging algorithm in Section~\ref{sec:Bagging}. 

In the context of this report, bootstrapping refers to sampling $N$ data points from the dataset $\left\{ \mathbf{x}_i, y_i \right\}_{i = 1}^N$, with replacement, and is only used for improving predictions. 
For an introduction to bootstrapping, see for instance \cite{efron1994bootstrap}.


\section{Variance of Random Forests regression}
\label{sec:Variance for Random Forests regression}
Let $T_i(\mathbf{x})$ denote a trained tree. The Random Forests prediction for $\mathbf{x}$ is,
\begin{align}
  \hat f(\mathbf{x}) = \frac{1}{B} \sum_{i = 1}^{B} T_i(\mathbf{x}).
\end{align}
The trees are created by sampling from the same empirical distributions, so they are identically distributed, with covariance,
\begin{align}
  \Cov[T_i(\mathbf{x}), T_j(\mathbf{x}))] = \rho \sigma^2, \quad \quad i \neq j.
\end{align}
Here $\sigma^2$ is the variance of a tree and $\rho$ is the correlation between two trees.  
The variance of $\hat f(\mathbf{x})$ is thus,
\begin{align}
\Var\left[ \frac{1}{B} \sum_{i = 1}^{B} T_i(\mathbf{x}) \right] 
&= \frac{1}{B^2} \sum_{i =1}^B \sum_{j= 1}^B \Cov[T_i(\mathbf{x}), T_j(\mathbf{x})] \notag \\
&= \frac{1}{B^2} \sum_{i =1}^B \left(\sum_{j \neq i} \Cov[T_i(\mathbf{x}), T_j(\mathbf{x})] + \Var[T_i(\mathbf{x})]  \right)\notag \\
&= \frac{1}{B^2} \sum_{i =1}^B \left( (B-1) \rho \sigma^2 + \sigma^2  \right)\notag \\
&= \rho \sigma^2 + \sigma^2 \frac{1-\rho}{B}.
\end{align}
So the variance decrease by increasing the number of bootstrap samples $B$ and by decorrelating the trees.

\chapter{Code from experiments}
\label{chap:Code}
Make sure to refer to this appendix in the experiments chapter.

<<readAll, cache=FALSE, eval=TRUE>>=
read_chunk('../code/spam/adaboostSpam.R')
read_chunk('../code/spam/adaboostPlot.R')
@
<<codeAdaboost, eval = FALSE>>=
@
<<plotAdaboost, eval = FALSE>>=
@
