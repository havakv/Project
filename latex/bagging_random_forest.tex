<<echo=FALSE, cache=FALSE>>=
set_parent('./project.Rnw')
@
\todo{Write something about how all these methods can be used for regression as well}
\section{Bagging}
\label{sec:Bagging}
Consider a scenario where a learning set $\left\{ \mathbf{x}_i, y_i \right\}_{i=1}^{N}$, is given, and a classifier $C(\mathbf{x})$ is created from this learning set. If a sequence of such sets are given, all drawn form the same distribution, one could get a more stable prediction by aggregating the classifiers. A typical way to do this is through the majority vote,
\begin{align}
  \label{eq:aggClass} 
  C_{agg}(\mathbf{x}) = \mathrm{majority} \left\{ C_b(\mathbf{x}) \right\}_{b=1}^{B}.
\end{align}
In 1996 Leo Breiman found that this process could be imitated on a single learning set by drawing \textit{bootstrap} samples \todo{referance here?}from the data. He called this method \textit{bagging}, or bootstrap aggregating \cite{Breiman1996}. 

The way it works is to draw $N$ samples (with replacement) from the data set $\left\{ (\mathbf{x}_i, y_i) \right\}_{i=1}^{N}$, and train a classifier $C_b(\mathbf{x})$ on the samples. Repeat this $B$ times and get the aggregated classifier in  \eqref{eq:aggClass}. The use of bootstrap samples makes the computations for each individual classifier independent. This mean that the training is easy to parallelize for faster computation. However, if the underlying method is interpretable, this will be lost in bagging.
\colorbox{yellow}{Background on bootstrapping? Se Efron and Tibshirani 1993.} \\
It is important to note that even though the votes gives proportions of sets predicting class $k$, these proportions should not be used at estimates for the class probabilities. Consider a two class experiment where the probability of class 1 is $0.75$. It is not unlikely that all the bagged classifiers predict class $1$, which gives a good aggregated classifier, but a horrible estimate of the class probability. However, often the underlying method used for each single classifier has a probability estimate (eg. trees). An alternative method to \eqref{eq:aggClass} is to average theses probabilities instead.
\begin{align}
  p_{agg, k}(\mathbf{x}) &=  \mathrm{ave}\left\{ p_{b, k}(\mathbf{x}) \right\}_{b = 1}^{B}, \quad k = 1, \ldots, K, \\
  \label{eq:aggClassP} 
  C_{agg}(\mathbf{x}) &= \argmax_k p_{agg, k}(\mathbf{x}).
\end{align}
This method is more descriptive than \eqref{eq:aggClass}, and has also been shown to often produce classifiers with lower variance, especially for small B's.\\
\colorbox{yellow}{Show example or refer to \cite[8.7]{modstat}.}
\\
\\
The typical argument for why this bootstrapping works, is how it lowers the variance of the predictor. This is easy to prove for regression \cite{modstat}, but the argument does not quite hold for classification, because of the non-additivity of bias and variance. The instability of the classifiers still plays an important role in the performance of bagging. For very stable methods it is clear that the classifiers for each bootstrap set will be very similar, so the gain of averaging or voting, will be minimal. However, for unstable methods bagging has be shown to perform very well. Among methods Breiman showed were unstable are neural networks, classification and regression trees, and subset selection in linear regression.

In \cite{Breiman1996} classification trees were fitted to a variety of dataset, and the bagged classifier showed a reduction in test set misclassification error ranging from $6\%$ to $77\%$. \todo{Should this be removed?} Trees are ideal to use because, if grown large, they can capture complex structures in the data, and have relatively low bias. A small change in the training set, can cause a tree to change completely, so their variance is high. Bagging manage to remove a lot of this variance. 

\subsection{Why bagging works}
\label{sub:Why bagging works}
\todo{Change $c$ to $y$ in this subsection}
This arguments is taken from \cite{Breiman1996}. Denote $y \in \left\{ 1, \ldots, K \right\}$ as the correct class for a given $\mathbf{x}$. Then the probability of correct classification for a classifier $C(\mathbf{x})$ given the predictor $\mathbf{x}$ is 
\begin{align}
  P(C(\mathbf{x}) = y \mid \mathbf{x})  &= \sum^{K}_{k=1} P(C(\mathbf{x})=y \medcap y = k \mid \mathbf{x}) \notag \\
  &= \sum^{K}_{k=1} P(C(\mathbf{x}) = y \mid y = k, \mathbf{x}) \cdot P(y=k \mid \mathbf{x}) \notag \\
  &= \sum^{K}_{k=1} P(C(\mathbf{x}) = k \mid \mathbf{x}) \cdot P(y=k \mid \mathbf{x}) 
\end{align}
where $P(C(\mathbf{x})=y \mid \mathbf{x})$ is the probability of $C$ predicting class $k$ for a certain $\mathbf{x}$, and $P(y=k \mid \mathbf{x})$ is the probability of class $k$ being the correct class for a certain $\mathbf{x}$. Thus the total probability of correct classification is 
\begin{align}
  P(C(\mathbf{x}) = y) = \int_{\mathbf{x}} \left(\sum^{K}_{k=1} P(C(\mathbf{x}) = k \mid \mathbf{x}) \cdot P(y=k \mid \mathbf{x})   \right) p(\mathbf{x}) d\mathbf{x},
\end{align}
where $p(\mathbf{x})$ is the probability distribution of $\mathbf{x}$. Observe that 
\begin{align}
  \sum^{K}_{k=1} P(C(\mathbf{x}) = k \mid \mathbf{x}) \cdot P(y=k \mid \mathbf{x}) \leq \max_k P(y = k \mid \mathbf{x}),
\end{align}
with equality if and only if 
\begin{align}
  P(C(\mathbf{x}) = k \mid \mathbf{x}) = \left\{ 
  \begin{array}{l l}
    1 & \quad \text{if } P(y = k \mid \mathbf{x}) = \max_j P(y = j \mid \mathbf{x})\\
    0 & \quad \text{else}
  \end{array} \right. .
\end{align}
The Bayes classifier, $C_{Bayes}(\mathbf{x}) = \argmax_k P(y = k \mid \mathbf{x})$, discussed earlier gives the expression above, and gives thus the lowes obtainable misclassification rate
\begin{align}
  \label{eq:bayesBagg} 
  P(C_{Bayes}(\mathbf{x}) = y) = \int_{\mathbf{x}} \max_k P(y=k \mid \mathbf{x})  p(\mathbf{x}) d\mathbf{x}.
\end{align}
$C(\mathbf{x})$ is called \textit{order-correct} if it follows the following criterion
\begin{align}
  \argmax_k P(C(\mathbf{x}) = k \mid \mathbf{x}) = \argmax_k  P(y = k \mid \mathbf{x}).
\end{align}
This means that if $\mathbf{x}$ gives class $k$ more than any other class, $C(\mathbf{x})$ predicts class $k$ more than any other class. This does not make $C(\mathbf{x})$ accurate as the probabilities $P(C(\mathbf{x}) = k \mid \mathbf{x}) $ and $P(y = k \mid \mathbf{x})$ can still be very different. For example consider the two class case where $P(y = 1 \mid \mathbf{x})  = 0.99$, while $P(C(\mathbf{x}) = 1\mid \mathbf{x}) = 0.51$, $C(\mathbf{x})$ is order correct, but not much better than pure guessing.

The aggregated predictors $C_{agg}$ in \eqref{eq:aggClass} and \eqref{eq:aggClassP} are approximations of  
\todo{Is this correct? NOT \eqref{eq:aggClassP}}
\begin{align}
  G(\mathbf{x}) = \argmax_k P(C(\mathbf{x})=k \mid \mathbf{x}).
\end{align}
Given $\mathbf{x}$, the probability for correct classification is 
\begin{align}
  P(G(\mathbf{x}) = y \mid \mathbf{x})  
  = \sum^{K}_{k=1} I\{ \argmax_j P(C(\mathbf{x}) = j \mid \mathbf{x}) = k \} P(y=k \mid \mathbf{x}).
\end{align}
If $C(\mathbf{x})$ is order-correct this becomes
\begin{align}
  P(G(\mathbf{x}) = y \mid \mathbf{x})  = \max_k P(y = k \mid \mathbf{x}),
\end{align}
and $G(\mathbf{x})$ will have the same probability of correct classification as the Bayes classifier in  \eqref{eq:bayesBagg}. Now let $H$ be the set of all inputs $\mathbf{x}$ for which $C(\mathbf{x})$ is order-correct, and $H^C$ be its compliment in terms of $C(\mathbf{x})$ not being order-correct. The probability of correct classification is now
\begin{align}
  P(G(\mathbf{x}) = y) = &\int_{\mathbf{x} \in H} \max_k P(y=k \mid \mathbf{x})  p(\mathbf{x}) d\mathbf{x} \quad + \\
  &\int_{\mathbf{x} \in H^C} \left(\sum^{K}_{k=1} I\{ G(\mathbf{x}) = k \} P(y=k \mid \mathbf{x})   \right) p(\mathbf{x}) d\mathbf{x}.
\end{align}
This shows that if a classifier is good in the sense that it is order-correct for most inputs $\mathbf{x}$, then the $G(\mathbf{x})$ is very good, and so should its approximation $C_{agg}(\mathbf{x})$ be. On the other hand, if $C(\mathbf{x})$ is not a good classifier, aggregating it can actually make it worse.
\\ \colorbox{yellow}{Mention that this only applies to the voting in \eqref{eq:aggClass}, not the averaging of probabilities \eqref{eq:aggClassP}. }

In \cite{domingos1997Bagging} the sucsess of bagging is explained through a different approach. Empirical experiments are used to validate the hypotheis that  
bagging reduces a classification learner's error rate because it changes the learner's model space and prior distribution to one that better fits the domain.


\subsection{Out-of-bag}
\label{sub:Out-of-bag}
\colorbox{yellow}{Maybe move this to bagging?}
When construction a bootstrap sample $B^* = \left\{ (\mathbf{x}_1^*, y_1^*), \ldots, (\mathbf{x}_n^*, y_n^*) \right\}$ the probability that $(\mathbf{x}_i^*, y_i^*) \neq (\mathbf{x}_j, y_j)$ is $\frac{n-1}{n}$. Thus the joint probability of $B^*$ not containing $(\mathbf{x}_j, y_j)$ is 
\begin{align}
  P\left(  (\mathbf{x}_j, y_j) \notin B^*\right) = \left( \frac{n-1}{n}  \right)^n 
  \xrightarrow{n \rightarrow \infty} e^{-1} \approx 0.37. 
\end{align}
Thus for large $n$ the point $(\mathbf{x}_j, y_j)$ will not be used to train $37 \%$ of the trees. Breiman suggested in 1996 \cite{outOfBag} that these points could be used to create an approximation of the misclassification error, with little extra cost. This is done the following way: For eache $(\mathbf{x}_i, y_i)$ construct a prediction $C(\mathbf{x}_i)$ of $y_i$ based only on the trees not trained on $(\mathbf{x}_i, y_i)$. Breiman called this the \textit{Out-of-bag} estimate or OOB. 

Test has shown that OOB gives an good approximations of the cross validated error \cite{outOfBag}, but without the large training cost. 
Hence, sequential classifiers based on bootstrap sampling, like bagging, stochastic gradient boosting \ref{sub:Stochastic gradient boosting} and random forest \ref{sec:Random forests}, can report the OOB for each iteration and terminate when the error stabilize. It can also be used instead of cross validation for tuining parameters.








\section{Random forests}
\label{sec:Random forests}
\todo{forests or forest?}
\todo{overfitt or overfit?}
%%fakesubsection Random forests
Hei \cite{randomforests}.
\\ \colorbox{yellow}{Boosting seems to outperform RF, but more tuning and takes longer to run?} \\
\colorbox{yellow}{RF and overfitting in modstat}\\
\url{http://www.montefiore.ulg.ac.be/~glouppe/pdf/phd-thesis.pdf} \\
\colorbox{yellow}{referance to random variable selection at each node. See phd. above p. 72} \\
\colorbox{yellow}{Write something about how boosting focus on reducing bias, while RF reduce variance?} \\
\\
In this section the method \textit{Random forests} proposed by Breiman \cite{randomforests} in 2001, will be investigated. The term \textit{random forests} is often used as a collection of methods building an ensemble of decision trees, grown in a randomized way, but only Breiman's method will be investigated here. 
\\\colorbox{yellow}{Other methods include ... see phd thesis p. 72}

The idea behind bagging was to reduce the instability of a classifier through aggregation of bootstrap samples. From a regression point of view, the success is is explained through variance reduction. Trees are very well suited for bagging, as they are  high variance and low bias. In 2001, Breiman suggested an improvement of this method, by reducing the variance further through de-correlating the trees in the method. He called this method \textit{Random forests} \cite{randomforests}.

The idea is very similar to bagging. The only difference is in the growing of a tree for each bootstrap sample. To grow a tree, each split should be done in the following fashion:
\begin{enumerate}
  \item Select $m$ predictors from the total amount of $p$ predictors ($\mathrm{dim}(\mathbf{x}) = p$).
  \item Do \textit{one} split, based on these $m$ variables (pick best variable and split-point). 
  \item Start over for the next split.
\end{enumerate}
Each tree is grown large and not pruned. Usually the stopping criterion is some minimum terminal node size.

Intuitively, the correlation between the trees should decrease by reducing $m$, but increase the variance of the single trees. Therefore $m$ should be considered a tuning parameter. However, the inventors recommend using $m = \lfloor \sqrt{p} \rfloor$, as a default value.
\\\colorbox{yellow}{Something about how few tuning parameters RF has compared to boosting}\\

\subsection{Why random forests works}
\label{sub:Why random forests works}
\url{https://stat.ethz.ch/education/semesters/ss2012/ams/slides/v10.2.pdf} \\
\colorbox{yellow}{The url has nice derivation of bias/variance tradeoff for regression (p. 6)}\\
\\
In section \ref{sub:Why bagging works} an argument was made for why aggregating bootstrap samples can improved the prediction power of a classifier. This argument is still valid for the random forest algorithm, but in this section the effect of de-correlating the trees will be investigated. As discussed earlier \ref{sub:Classification trees}, the bias-variance tradeoff from regression \ref{sub:Bias-variance tradeoff} is sort of applicable to classification as well. It is easier to study the effect of de-correlating  the trees in the regression framework, so this will be presented first.

\subsubsection{Regression}
\label{sub:Regression}

Let $T_i(\mathbf{x})$ denote a trained tree. The random forest prediction for $\mathbf{x}$ is the average prediction over all the $B$ trees,
\begin{align}
  \hat f(\mathbf{x}) = \frac{1}{B} \sum_{i = 1}^{B} T_i(\mathbf{x}).
\end{align}
Let $\sigma$ denote the variance of a tree, and $\rho$ the correlation between trees. In this argument, both parameters are assumed to be the same for all trees. In Appendix~\ref{sub:Variance for random forest regression} it is shown that the variance of the prediction is, 
\begin{align}
\label{eq:RFVarReg} 
  \Var[\hat f(\mathbf{x})] = \rho \sigma^2 + \sigma^2 \frac{1-\rho}{B}.
\end{align}
As discussed earlier, large trees are considered relatively unbiased, but with high variance. As the bias is a linear operator, the bias of the ensemble $\hat f(\mathbf{x})$ is the same as for an individual tree $T_i(\mathbf{x})$. From Eq.~\eqref{eq:RFVarReg} it is clear that the second term vanish as $B$ grows, thus bagging and random forest manage to reduce the variance of a method without increasing the bias. To further reduce the variance the first term in in Eq.~\eqref{eq:RFVarReg} must be reduced, which can be accomplished by reducing the correlation $\rho$ between the trees. 

\subsubsection{Classification}
\label{sub:Classification}
In he's paper about random forest \cite{randomforests}, Breiman shows that for $0/1$ loss classification, the generalization error for random forests is composed by the strength of the individual classifiers, and the correlation between classifiers in terms of raw margin functions. The following will in principle be a slightly rewritten version of his argument.

A margin function for an ensemble of trees is given by
\begin{align}
  mg(\mathbf{x}, y) = \mathrm{ave}_k I\left\{ T_k(\mathbf{x}, \theta) = y \right\} - 
  \max_{j \neq y} \mathrm{ave}_k I\left\{ T_k(\mathbf{x}, \theta) = j \right\}.
\end{align}
Here $T_k(\mathbf{x}, \theta)$ is a single tree in the forest, and $\theta$ represents its parameters. 
The margin function thus measure the extent to which the vote for the right class exceeds any other class, and is a useful measure for performance of a classifier given point $(\mathbf{x}, y)$.  
In \ref{sub:Overfitting} it is shown that the margin function for a random forest is a good approximation of
\begin{align}
  mr(\mathbf{x}, y) = P_{\theta} (T(\mathbf{x}, \theta) = y) - \max_{j \neq y} P_{\theta}(T(\mathbf{x}, \theta) = j), 
\end{align}
for a large forest. The generalization error is given by
\begin{align}
  PE^* = P_{\mathbf{x}, y}(mr(\mathbf{x}, y) < 0),
\end{align}
and gives a performance measure over the probabilities for $\mathbf{x}$ and $y$. It can be read as the probability for misclassification, and is gives the same value as the expected prediction error $\mathrm{EPE}$ for $0/1$-loss discussed earlier. 
The strength of a set of classifiers $\left\{ T(\mathbf{x}, \theta) \right\}$ is defined as
\begin{align}
  s = \E_{\mathbf{x}, y} [mr(\mathbf{x}, y)]. 
\end{align}
Assume that for the rest of this section that $s \geq 0$. 

According to Chebyshev's inequality 
\begin{align}
  P_{\mathbf{x}, y} ([mr(\mathbf{x}, y)-s]^2 \geq s^2) \leq 
  \frac{\E_{\mathbf{x},y}[(mr(\mathbf{x}, y) - s)^2)]}{s^2} =
  \frac{\Var[mr(\mathbf{x}, y)]}{s^2}.
\end{align}
\begin{align}
  P_{\mathbf{x}, y} ([mr(\mathbf{x}, y)-s]^2 \geq s^2)
  &= P_{\mathbf{x}, y} (mr(\mathbf{x}, y)^2 \geq mr(\mathbf{x}, y)\: s) \notag \\
  &= P_{\mathbf{x}, y} (mr(\mathbf{x}, y) \geq s  \cup mr(\mathbf{x}, y) \leq 0) \notag \\
  & \geq P_{\mathbf{x}, y} (mr(\mathbf{x}, y) < 0)
\end{align}
This mean a connection between the generalization error and the variance of the method is
\begin{align}
  PE^* \leq  \frac{\Var[mr(\mathbf{x}, y)]}{s^2}.
\end{align}

Now let us simplify notation by defining
\begin{align}
  \hat j(\mathbf{x}, y) 
\end{align}
<++>





\colorbox{yellow}{I think it was something about this in modstat.}
\\\colorbox{yellow}{Do the actual analysis from the original paper. }

\subsection{Overfitting}
\label{sub:Overfitting}
\colorbox{yellow}{Write this section when done with ''Why random forrests work''}
\\\colorbox{yellow}{The same counts for bagging, but the proof is from RF paper.}
\\\colorbox{yellow}{See p. 596 in modstat on how tree depth can overfitt.}



\subsection{Tuning}
\label{sub:Tuning}
\colorbox{yellow}{Something about tuning, and how this is easier to do than for boosting.}\\
One of the reasons why random forest is popular is that it is relatively easy to tune. As discussed above, it does not overfit as the number of bootstrap samples $B$ increase. So $B$ can either just be set high, or one can use the OOB error as a stopping criterion on number of iterations (see Section~\ref{sub:Out-of-bag}). It was suggested that the tree depth could be tuned, but in \cite{modstat}'s experience fully grown trees is usually good and results in one less tuning parameter. 

The number of splitting variables $m$ is however an important tuning parameter. As mentioned earlier, $m = \lfloor \sqrt{p} \rfloor$, is a good default value and the optimal $m$ is often close to this value. 

So tuning random forest can often just be on one variable, $m$, that has a good default value. Comparing this to for instance gradient boosting, from Section~\ref{sec:Boosting}, the random forest algorithm might be the preferred out-of-the-box classifier. 


\subsection{Robustness and overfitting}
\label{sub:Robustness and overfitting}
\colorbox{yellow}{Say something about how adding noise compares to adding noise in boosting?}
\\ \colorbox{yellow}{Maybe overfitting is covered enough in the Why random forests work?}

\subsection{Variable importance}
\label{sub:Variable importance}
\colorbox{yellow}{Should I write about this? In that case, should do the same for boosting.}
