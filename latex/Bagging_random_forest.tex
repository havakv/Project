<<echo=FALSE, cache=FALSE>>=
set_parent('./project.Rnw')
@
\section{Deletesec}

The term \textit{random forests} is often used as a collection of methods averaging an ensemble of decision trees grown in a randomized way. 
Some of the most well known methods include Bagging by \cite{Breiman1996}, \textit{Random Subspace Method} by \cite{ho1998random}, 
\textit{Random Forests} \footnote{\textit{Random Forests} with capitals refers to the specific method by \cite{randomforests}, while \textit{random forests} refers to the class of methods} by \cite{randomforests},
\textit{Perfect Random Tree Ensembles} by \cite{cutler2001pert}, \textit{Extremely Randomized Trees} by \cite{Geurts2006} and \textit{Rotation Forests} by \cite{Rodriguez2006}. 
In this report only Bagging and Random Forests will be discussed.

\section{Bagging}
\label{sec:Bagging}
Consider a scenario where a learning set $\left\{ \mathbf{x}_i, y_i \right\}_{i=1}^{N}$, is given, and a classifier $C(\mathbf{x})$ is created from the set. If a sequence of such sets are given, all drawn form the same distribution, a better prediction could be obtained by aggregating the classifiers. A typical way to do this is through the majority vote,
\begin{align}
  \label{eq:aggClass} 
  C_{agg}(\mathbf{x}) = \mathrm{majority} \left\{ C_b(\mathbf{x}) \right\}_{b=1}^{B}.
\end{align}
\cite{Breiman1996} found that this process could be imitated on a single learning set by drawing bootstrap samples from the data (see Appendix~\ref{sec:Bootstrapping} for background on bootstrapping). He called this method Bagging, or bootstrap aggregating. 

Bagging is done by sampling $N$ points (with replacement) from the data set $\left\{ \mathbf{x}_i, y_i \right\}_{i=1}^{N}$, and train a classifier $C_b(\mathbf{x})$ on the samples. Repeat this $B$ times and get the aggregated classifier in  \eqref{eq:aggClass}. 

The use of bootstrap samples makes the computations for each individual classifier independent. This mean that the training is easy to parallelize for faster computation. However, if the underlying method is interpretable, this will be lost in Bagging.
\\
\\
It is important to note that even though the votes gives proportions of classifiers predicting class $k$, these proportions should not be used as estimates for the class probabilities. This can be shown through an easy example. Consider a two class experiment where the probability of class 1 is $p = 0.75$, and the features are pure noise. The prediction of $C_b(\mathbf{x})$ thus only depends on how many points there is from each class. As the draws in a bootstrap sample are done with replacement, the amount of points with class $1$ in sample ($\# 1$) is binomially distributed. The probability that $C_b(\mathbf{x})$ gives class 2 is thus, 
\begin{align}
  P(C_b(\mathbf{x})=2) = P\left(\#1 <  \frac{N}{2} \right) = \sum_{i = 0}^{N/2 - 1} \mathrm{binom}\left(\frac{N}{2}-1,\: N,\: p  \right).
\end{align}
This probability will converge to 0 as $N$ increase, for all probabilities $p > 0.5$. Thus the proportion of $C_b(\mathbf{x})$ that votes for class 1 is a horrible estimator of $p$.

However, often the underlying method used for each single classifier has a probability estimate (eg. trees). An alternative method to \eqref{eq:aggClass} is to average these probabilities instead,
\begin{align}
  p_{agg, k}(\mathbf{x}) &=  \mathrm{ave}\left\{ p_{b, k}(\mathbf{x}) \right\}_{b = 1}^{B}, \quad k = 1, \ldots, K, \\
  \label{eq:aggClassP} 
  C_{agg}(\mathbf{x}) &= \argmax_k p_{agg, k}(\mathbf{x}).
\end{align}
This method is more descriptive than \eqref{eq:aggClass}, and has also, according to \cite{modstat}, been shown to often produce classifiers with lower variance, especially for small B's.

\subsection{Why Bagging works}
\label{sub:Why Bagging works}
The typical argument for the success of Bagging is that it lowers the variance of the classifier through averaging. This can be shown for regression (see Appendix~\ref{sec:Variance for Random Forests regression}), but the argument does not quite hold for classification, because of the non-additivity of bias and variance. The instability of the classifiers still plays an important role in the performance of Bagging. For very stable methods it is clear that the classifiers for each bootstrap set will be very similar, so the gain of averaging or voting, will be minimal. However, for unstable methods Bagging has been shown to perform very well. \cite{Breiman1996Heur} showed that neural networks, classification and regression trees, and subset selection in linear regression are relatively unstable.
Trees are ideal to use because, if grown large, they can capture complex structures in the data, and have relatively low bias. A small change in the training set, can cause a tree to change completely, so their variance is high. Bagging manage to remove a lot of this variance. Because of the success of using trees, Bagging is considered a tree based algorithm.
\\
\\
A more thorough argument for Bagging as a classifier was made by \cite{Breiman1996}. Denote $y \in \left\{ 1, \ldots, K \right\}$ as the correct class for a given $\mathbf{x}$. Then the probability of correct classification for a classifier $C(\mathbf{x})$ given the predictor $\mathbf{x}$ is,
\begin{align}
  P(C(\mathbf{x}) = y \mid \mathbf{x})  &= \sum^{K}_{k=1} P(C(\mathbf{x})=y \medcap y = k \mid \mathbf{x}) \notag \\
  &= \sum^{K}_{k=1} P(C(\mathbf{x}) = y \mid y = k, \mathbf{x}) \cdot P(y=k \mid \mathbf{x}) \notag \\
  &= \sum^{K}_{k=1} P(C(\mathbf{x}) = k \mid \mathbf{x}) \cdot P(y=k \mid \mathbf{x}), 
\end{align}
where $P(C(\mathbf{x})=y \mid \mathbf{x})$ is the probability of $C$ predicting class $k$ for a certain $\mathbf{x}$, and $P(y=k \mid \mathbf{x})$ is the probability of class $k$ being the correct class for a certain $\mathbf{x}$. Thus the total probability of correct classification is,
\begin{align}
  P(C(\mathbf{x}) = y) = \int_{\mathbf{x}} \left(\sum^{K}_{k=1} P(C(\mathbf{x}) = k \mid \mathbf{x}) \cdot P(y=k \mid \mathbf{x})   \right) p(\mathbf{x}) d\mathbf{x},
\end{align}
where $p(\mathbf{x})$ is the probability distribution of $\mathbf{x}$. Observe that, 
\begin{align}
  \sum^{K}_{k=1} P(C(\mathbf{x}) = k \mid \mathbf{x}) \cdot P(y=k \mid \mathbf{x}) \leq \max_k P(y = k \mid \mathbf{x}),
\end{align}
with equality if and only if,
\begin{align}
  P(C(\mathbf{x}) = k \mid \mathbf{x}) = \left\{ 
  \begin{array}{l l}
    1 & \quad \text{if } P(y = k \mid \mathbf{x}) = \max_j P(y = j \mid \mathbf{x})\\
    0 & \quad \text{else}
  \end{array} \right. .
\end{align}
The Bayes classifier $C_{Bayes}(\mathbf{x}) = \argmax_k P(y = k \mid \mathbf{x})$, discussed earlier, gives the expression above, and gives thus the lowers obtainable misclassification rate,
\begin{align}
  \label{eq:bayesBagg} 
  P(C_{Bayes}(\mathbf{x}) = y) = \int_{\mathbf{x}} \max_k P(y=k \mid \mathbf{x})  p(\mathbf{x}) d\mathbf{x}.
\end{align}
$C(\mathbf{x})$ is called \textit{order-correct} if it follows the following criterion,
\begin{align}
  \argmax_k P(C(\mathbf{x}) = k \mid \mathbf{x}) = \argmax_k  P(y = k \mid \mathbf{x}).
\end{align}
This means that if $\mathbf{x}$ gives class $k$ more than any other class, $C(\mathbf{x})$ predicts class $k$ more than any other class. This does not make $C(\mathbf{x})$ accurate, as the probabilities $P(C(\mathbf{x}) = k \mid \mathbf{x}) $ and $P(y = k \mid \mathbf{x})$ can still be very different. For example consider the two class case where $P(y = 1 \mid \mathbf{x})  = 0.99$, while $P(C(\mathbf{x}) = 1\mid \mathbf{x}) = 0.51$, $C(\mathbf{x})$ is order correct, but not much better than pure guessing.

The aggregated predictor $C_{agg}$ in \eqref{eq:aggClass} is an approximation of, 
\begin{align}
  G(\mathbf{x}) = \argmax_k P(C(\mathbf{x})=k \mid \mathbf{x}).
\end{align}
Given $\mathbf{x}$, the probability for correct classification is,
\begin{align}
  P(G(\mathbf{x}) = y \mid \mathbf{x})  
  = \sum^{K}_{k=1} I\{ \argmax_j P(C(\mathbf{x}) = j \mid \mathbf{x}) = k \} P(y=k \mid \mathbf{x}).
\end{align}
If $C(\mathbf{x})$ is order-correct this becomes,
\begin{align}
  P(G(\mathbf{x}) = y \mid \mathbf{x})  = \max_k P(y = k \mid \mathbf{x}),
\end{align}
and $G(\mathbf{x})$ will have the same probability of correct classification as the Bayes classifier in  \eqref{eq:bayesBagg}. 

Now let $H$ be the set of all inputs $\mathbf{x}$ for which $C(\mathbf{x})$ is order-correct, and $H^C$ be its compliment in terms of $C(\mathbf{x})$ not being order-correct. The probability of correct classification can now be expressed as,
\begin{align}
  P(G(\mathbf{x}) = y) = &\int_{\mathbf{x} \in H} \max_k P(y=k \mid \mathbf{x})  p(\mathbf{x}) d\mathbf{x} \quad + \\
  &\int_{\mathbf{x} \in H^C} \left(\sum^{K}_{k=1} I\{ G(\mathbf{x}) = k \} P(y=k \mid \mathbf{x})   \right) p(\mathbf{x}) d\mathbf{x}.
\end{align}
This shows that if a classifier is good in the sense that it is order-correct for most inputs $\mathbf{x}$, then $G(\mathbf{x})$ is very good, and so should its approximation $C_{agg}(\mathbf{x})$ be. On the other hand, if $C(\mathbf{x})$ is not a good classifier, aggregating it can actually make it worse.

In \cite{domingos1997Bagging} the another attempt on explaining Bagging's success is made. Empirical experiments are used to validate the hypothesis that Bagging reduces a classification learner's error rate because it changes the learner's model space and prior distribution to one that better fits the domain. 


\subsection{Out-of-bag}
\label{sub:Out-of-bag}
When constructing a bootstrap sample $B^* = \left\{ (\mathbf{x}_1^*, y_1^*), \ldots, (\mathbf{x}_N^*, y_N^*) \right\}$ the probability that $(\mathbf{x}_i^*, y_i^*) \neq (\mathbf{x}_j, y_j)$ is $\frac{N-1}{N}$. Thus the joint probability of $B^*$ not containing $(\mathbf{x}_j, y_j)$ is 
\begin{align}
  P\left(  (\mathbf{x}_j, y_j) \notin B^*\right) = \left( \frac{N-1}{N}  \right)^N
  \xrightarrow{N \rightarrow \infty} e^{-1} \approx 0.37. 
\end{align}
Thus for large $N$ the point $(\mathbf{x}_j, y_j)$ will not be used to train $37 \%$ of the trees. \cite{outOfBag} suggested that these points could be used to create an approximation of the misclassification error. This is done the following way: For each $(\mathbf{x}_i, y_i)$ construct a prediction $C(\mathbf{x}_i)$ for $y_i$ based only on the trees not trained on $(\mathbf{x}_i, y_i)$. Then find the misclassification error. Breiman called this the \textit{Out-of-bag} error estimate or OOB. 

\cite{outOfBag}, validated empirically that OOB gives good approximations of the cross-validated error, but without the large training cost. 
Hence, sequential classifiers based on bootstrap sampling, like Bagging, Stochastic Gradient Boosting and Random Forests, can report the OOB for each iteration and terminate when the error stabilize. It can also be used instead of cross-validation for tuning parameters.


\section{Random Forests}
\label{sec:Random Forests}
%%fakesubsection Random Forests
Random Forests by \cite{randomforests} is an extension of his Bagging algorithm. By introducing more randomization, it is suppose to further reduce the variance through decorrelating the trees. 

The method is very similar to Bagging. They only differ in how the individual trees are grown. Bootstrap samples are drawn in the same fashion as for Bagging. Then, for the creation of each split in a tree, the following steps are done. 
\begin{enumerate}
  \item Select $m$ predictors at random from the total amount of $p$ predictors ($\mathrm{dim}(\mathbf{x}) = p$).
  \item Do \textit{one} split, based on these $m$ variables (pick best variable and split-point). 
\end{enumerate}
Each tree is grown large and not pruned. Usually the stopping criterion is some minimum terminal node size.

Intuitively, the correlation between the trees should decrease by reducing $m$, but increase the variance of the individual trees. Therefore $m$ should be considered a tuning parameter. The inventors recommend using $m = \lfloor \sqrt{p} \rfloor$ as a default value.

\subsection{Why Random Forests works}
\label{sub:Why Random Forests works}
In section \ref{sub:Why Bagging works} an argument was made for why aggregating bootstrap samples can improved the prediction power of a classifier. This argument is still valid for the Random Forests algorithm, but in this section the effect of decorrelating the trees will be investigated. As discussed in \ref{sec:Tree}, the bias-variance tradeoff from regression is sort of applicable to classification as well. It is easier to study the effect of decorrelating  the trees in the regression framework, so this will be presented first.

\subsubsection{Regression}
\label{sub:Regression}

Let $T_i(\mathbf{x})$ denote a trained tree. The Random Forest prediction for $\mathbf{x}$ is the average prediction over all the $B$ trees,
\begin{align}
  \hat f(\mathbf{x}) = \frac{1}{B} \sum_{i = 1}^{B} T_i(\mathbf{x}).
\end{align}
As the trees are created from the same distribution, they are identically distributed. Let $\sigma^2$ denote the variance of a tree, and $\rho$ the correlation between trees. In Appendix~\ref{sec:Variance for Random Forests regression} it is shown that the variance of the prediction is, 
\begin{align}
\label{eq:RFVarReg} 
  \Var[\hat f(\mathbf{x})] = \rho \sigma^2 + \sigma^2 \frac{1-\rho}{B}.
\end{align}
Recall that large trees are considered relatively unbiased, but with high variance. As the bias is a linear operator, the bias of the ensemble $\hat f(\mathbf{x})$ is the same as for an individual tree $T_i(\mathbf{x})$. From \eqref{eq:RFVarReg} it is clear that the second term vanish as $B$ grows, thus Bagging and Random Forests manage to reduce the variance of a method without increasing the bias. To further reduce the variance the first term in \eqref{eq:RFVarReg} must be reduced, which can be accomplished by reducing the correlation $\rho$ between the trees. 

\subsubsection{Classification}
\label{sub:Classification}
For classification there is no equivalent measure to variance of an individual classifiers. One approach is therefore to construct a function based on the classifier and use the variance of this as a substitute for the variance of the classifier. 
\cite{randomforests} suggested one such approach. He showed that for Random Forests classification, a connection could be made between the \textit{generalization error}, the \textit{strength} of the individual classifiers and the correlation between classifiers in terms of raw margin functions. 

A margin function for an ensemble of trees is given by,
\begin{align}
  mg(\mathbf{x}, y) = \mathrm{ave}_k I\left\{ T_k(\mathbf{x}) = y \right\} - 
  \max_{j \neq y} \mathrm{ave}_k I\left\{ T_k(\mathbf{x}) = j \right\}.
\end{align}
Here $T_k(\mathbf{x})$ is a single tree in the forest. 
The margin function thus measure the extent to which the vote for the right class exceeds any other class, and is a useful measure for performance of a classifier in point $(\mathbf{x}, y)$.  
In \ref{sub:Overfitting} it is shown that the margin function for Random Forests is a good approximation of,
\begin{align}
  mr(\mathbf{x}, y) = P_{\bm \theta} (T(\mathbf{x}, \bm \theta) = y) - \max_{j \neq y} P_{\bm \theta}(T(\mathbf{x}, \bm \theta) = j), 
\end{align}
where $\bm \theta$ represents a trees parameters.
The generalization error is a performance measure for the classifier, and is given by,
\begin{align}
  PE^* = P_{\mathbf{x}, y}(mr(\mathbf{x}, y) < 0),
\end{align}
It can be read as the probability for misclassification, and takes the same value as the expected prediction error $\mathrm{EPE}$ under $0/1$ loss. 
Let the strength of a set of classifiers $\left\{ T(\mathbf{x}, \bm \theta) \right\}$ be defined as,
\begin{align}
  s = \E_{\mathbf{x}, y} [mr(\mathbf{x}, y)],
\end{align}
and assume for the rest of this section that $s \geq 0$.  
According to Chebyshev's inequality,
\begin{align}
  P_{\mathbf{x}, y} ([mr(\mathbf{x}, y)-s]^2 \geq s^2) \leq 
  \frac{\E_{\mathbf{x},y}[(mr(\mathbf{x}, y) - s)^2)]}{s^2} =
  \frac{\Var[mr(\mathbf{x}, y)]}{s^2},
\end{align}
and,
\begin{align}
  P_{\mathbf{x}, y} ([mr(\mathbf{x}, y)-s]^2 \geq s^2)
  &= P_{\mathbf{x}, y} (mr(\mathbf{x}, y)^2 \geq mr(\mathbf{x}, y)\: s) \notag \\
  &= P_{\mathbf{x}, y} (mr(\mathbf{x}, y) \geq s  \cup mr(\mathbf{x}, y) \leq 0) \notag \\
  & \geq P_{\mathbf{x}, y} (mr(\mathbf{x}, y) < 0).
\end{align}
This means a connection between the generalization error and the variance of the method is,
\begin{align}
  \label{eq:PEVar} 
  PE^* \leq  \frac{\Var[mr(\mathbf{x}, y)]}{s^2}.
\end{align}
It it thus clear that for higher strength of individual trees and lower variance of the ensemble, the methods performance will improve. This should not come as a surprise. 

Further, the variance is investigated to explicitly find a connection to the correlation between the margin functions.  
First, let us simplify notation by defining,
\begin{align}
  \hat j(\mathbf{x}, y) = \argmax_{j \neq y} P_{\bm \theta}(T(\mathbf{x}, \bm \theta) = j).
\end{align}
The margin function can now be written as,
\begin{align}
  mr(\mathbf{x},y) 
  &= P_{\bm \theta}(T(\mathbf{x},\bm \theta) = y) - P_{\bm \theta} (T(\mathbf{x}, \bm \theta) = \hat j (\mathbf{x}, y)) \notag \\
  &= \E_{\bm \theta}[I\{T(\mathbf{x},\bm \theta) = y\} - I\{T(\mathbf{x}, \bm \theta) = \hat j (\mathbf{x}, y)\}],
\end{align}
where,
\begin{align}
  rmg(\bm \theta, \mathbf{x}, y) = I\{T(\mathbf{x},\bm \theta) = y\} - I\{T(\mathbf{x}, \bm \theta) = \hat j (\mathbf{x}, y)\},
\end{align}
is called the \textit{raw margin function}.

Let $\bm \theta$ and $\bm \theta'$ be independent identically distributed, and let $f$ be an arbitrary function. The following identity is useful,
\begin{align}
  \E_{\bm \theta}[f(\bm \theta)]^2 = \E_{\bm \theta, \bm \theta'}[f(\bm \theta) f(\bm \theta')]. 
\end{align}
This gives that,
\begin{align}
  mr(\mathbf{x}, y)^2  = \E_{\bm \theta, \bm \theta'}[rmg(\bm \theta, \mathbf{x}, y) \: rmg(\bm \theta', \mathbf{x}, y)]. 
\end{align}
The variance in \eqref{eq:PEVar} can now be rewritten the following way,
\begin{align}
  \Var(mr(\mathbf{x}, y)) 
  &= \E_{\mathbf{x}, y}[mr(\mathbf{x}, y)^2] - s^2 \notag \\
  &= \E_{\mathbf{x}, y}[ \E_{\bm \theta, \bm \theta'}[rmg(\bm \theta, \mathbf{x}, y) \: rmg(\bm \theta', \mathbf{x}, y)]] - s^2.
\end{align}
The covariance of the raw margin functions can be expressed as,
\begin{align}
  \Cov_{\mathbf{x}, y} [rmg(\bm \theta, \mathbf{x}, y), rmg(\bm \theta', \mathbf{x}, y)] &= \notag \\
  \E_{\mathbf{x}, y} [rmg(\bm \theta, \mathbf{x}, y) \: rmg(\bm \theta', \mathbf{x}, y)] &-
  \E_{\mathbf{x}, y} [rmg(\bm \theta, \mathbf{x}, y)]\: \E_{\mathbf{x}, y} [rmg(\bm \theta', \mathbf{x}, y)].
\end{align}
Now, assuming the order of expectations are interchangeable, the variance becomes,
\begin{align}
  \Var(mr(\mathbf{x}, y)) 
  = &\E_{\bm \theta, \bm \theta'}[\E_{\mathbf{x}, y}[ rmg(\bm \theta, \mathbf{x}, y) \: rmg(\bm \theta', \mathbf{x}, y)]] - s^2 \notag \\
  = &\E_{\bm \theta, \bm \theta'}[\Cov_{\mathbf{x}, y} [rmg(\bm \theta, \mathbf{x}, y), rmg(\bm \theta', \mathbf{x}, y)]] \: +\notag \\
    &\E_{\bm \theta, \bm \theta'} [\E_{\mathbf{x}, y} [rmg(\bm \theta, \mathbf{x}, y)]\: \E_{\mathbf{x}, y} [rmg(\bm \theta', \mathbf{x}, y)]]
  - s^2 \notag \\
  \label{eq:VarCovRF} 
  = &\E_{\bm \theta, \bm \theta'}[\Cov_{\mathbf{x}, y} [rmg(\bm \theta, \mathbf{x}, y), rmg(\bm \theta', \mathbf{x}, y)]].
\end{align}
The last equations comes from the fact that $\bm \theta$ and $\bm \theta'$ are independent identically distributed,
\begin{align}
  \E_{\bm \theta, \bm \theta'} [\E_{\mathbf{x}, y} [rmg(\bm \theta, \mathbf{x}, y)]\: \E_{\mathbf{x}, y} [rmg(\bm \theta', \mathbf{x}, y)]] 
  &= \E_{\mathbf{x}, y} [\E_{\bm \theta} [rmg(\bm \theta, \mathbf{x}, y)] \: \E_{\bm \theta'}[rmg(\bm \theta', \mathbf{x}, y)]] \notag \\
  &= \E_{\mathbf{x}, y} [\E_{\bm \theta} [rmg(\bm \theta, \mathbf{x}, y)]^2]  \notag \\
  &= s^2.
\end{align}
Introducing $\rho(\bm \theta, \bm \theta')$ as the correlation between $rmg(\bm \theta, \mathbf{x}, y)$ and $rmg(\bm \theta', \mathbf{x}, y)$ holding $\bm \theta$, $\bm \theta'$ fixed, and let $sd(\bm \theta)$ be the standard deviation of $rmg(\bm \theta, \mathbf{x}, y)$ holding $\bm \theta$ fixed. \eqref{eq:VarCovRF} can now be expressed as,
\begin{align}
  \Var[mr(\mathbf{x}, y)] 
  &= \E_{\bm \theta, \bm \theta'}[\rho(\bm \theta, \bm \theta') \: sd(\bm \theta) \: sd(\bm \theta')] \notag \\
  &= \bar \rho \: \E_{\bm \theta}[sd(\bm \theta)]^2 \notag \\
  \label{eq:RFVarMr} 
  &\overset{\text{Jensen}}{\leq}  \bar \rho \: \E_{\bm \theta}[\Var_{\mathbf{x}, y}[rmg(\bm \theta, \mathbf{x}, y)]],
\end{align}
where $\bar \rho$ is the mean of the correlation, defined by,
\begin{align}
  \bar \rho = 
  \frac{\E_{\bm \theta, \bm \theta'}[\rho(\bm \theta, \bm \theta') \: sd(\bm \theta) \: sd(\bm \theta')]}
  {\E_{\bm \theta, \bm \theta'}[sd(\bm \theta) \: sd(\bm \theta')]}.
\end{align}
Further, the expectation of the variance of the raw margin function has the following relationship with the strength $s$,
\begin{align}
  \E_{\bm \theta}[\Var_{\mathbf{x}, y}[rmg(\bm \theta, \mathbf{x}, y)]] 
  &= \E_{\bm \theta}[\E_{\mathbf{x}, y}[rmg(\bm \theta, \mathbf{x}, y)^2]] - \E_{\bm \theta}[\E_{\mathbf{x}, y}[rmg(\bm \theta, \mathbf{x}, y)]^2] \notag \\
  &\overset{\text{Jensen}}{\leq}
  \E_{\bm \theta}[\E_{\mathbf{x}, y}[rmg(\bm \theta, \mathbf{x}, y)^2]] - \E_{\mathbf{x}, y}[\E_{\bm \theta}[rmg(\bm \theta, \mathbf{x}, y)]]^2 \notag \\
  &= \E_{\bm \theta}[\E_{\mathbf{x}, y}[rmg(\bm \theta, \mathbf{x}, y)^2]] - s^2 \notag \\
  \label{eq:RF1mins2} 
  &\leq 1 - s^2,
\end{align}
as $rmg(\bm \theta, \mathbf{x}, y) \in \left\{ -1, 1 \right\}$. Now combining \eqref{eq:RF1mins2}, \eqref{eq:RFVarMr} and \eqref{eq:PEVar} yields the bound,
\begin{align}
  PE^* \leq \bar \rho  \frac{1 - s^2}{s^2} .
\end{align}
So Breiman managed to create a bound for the generalization error only based on the strength $s$ of the individual classifiers, and the correlation $\bar \rho$ between them in terms of raw margin functions. 

For the two class case with $y \in \left\{ -1, 1 \right\}$ the expression simplifies. The raw margin function become $2 I\left\{ T(\mathbf{x}, \bm \theta) = y \right\} - 1$, and $\bar \rho$ becomes the correlation between the trees,
\begin{align}
  \bar \rho = \E_{\bm \theta, \bm \theta'}[\rho(T(\mathbf{x}, \bm \theta), T(\mathbf{x}, \bm \theta'))]. 
\end{align}
This suggest that by decorrelating the trees, the prediction error will decrease. 



\subsection{Overfitting}
\label{sub:Overfitting}
Following the reasoning in \ref{sub:Why Random Forests works}, the margin function for an ensemble of trees is,
\begin{align}
  mg(\mathbf{x}, y) = \mathrm{ave}_k I\left\{ T_k(\mathbf{x}) = y \right\} - 
  \max_{j \neq y} \mathrm{ave}_k I\left\{ T_k(\mathbf{x}) = j \right\}.
\end{align}
As the trees in Random Forests are identically distributed, $T_k(\mathbf{x})$ is a function of a set of parameters $\bm \theta$, so $T_k(\mathbf{x}) = T(\mathbf{x}, \bm \theta)$. \cite{randomforests} proves that as the number of trees increase, for almost surely all sequences $\bm \theta_1, \bm \theta_2, \ldots$, $PE^*$ converges to 
\begin{align}
  P_{\mathbf{x}, y} (P_{\bm \theta} (T(\mathbf{x}, \bm \theta) = y) - \max_{j \neq y} P_{\bm \theta}(T(\mathbf{x}, \bm \theta) = j) < 0).
\end{align}
This shows that Random Forests does not overfitt as the number of trees increase. The same argument of course counts for Bagging. 

From \ref{sub:Why Random Forests works} it is clear that the strength of the individual trees plays a major part in the performance of the ensemble. They, however, can be overfitted. \cite{segal2004} showed this by controlling the depth of the individual trees in Random Forests regression, and experiencing small gains. 


\subsection{Tuning}
\label{sub:Tuning}
One of the reasons why Random Forests is popular is that it is relatively easy to tune. As discussed above, it does not overfit as the number of bootstrap samples $B$ increase. So $B$ can either just be set high, or one can for instance use the OOB error as a stopping criterion on number of iterations (see Section~\ref{sub:Out-of-bag}). It was suggested that the tree depth could be tuned, but \cite{modstat} argue that fully grown trees usually works just fine and result in one less tuning parameter. 

The number of splitting variables $m$, however, is  important to consider. As mentioned earlier, $m = \lfloor \sqrt{p} \rfloor$, is a good default value and the optimal $m$ is often close to this value. 
So tuning Random Forests can often be do by considering \textit{one} variable, $m$, that has a good default value. Comparing this to for instance Gradient Boosting, the Random Forests algorithm might be the preferred out-of-the-box classifier. 

