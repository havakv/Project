\section{Tree-based methods}
\label{sec:Tree-based methods}
\todo{Is boosting a tree method?}
\colorbox{yellow}{Change name of section if boosting is not a tree-based method}\\
In Section~\ref{sec:Linear Classifiers}, only very basic linear methods for classification were discussed. The log odds were modeled in a linear fashion and a decision boundary was drawn based on this. In this section the framework is completely different. The linearity assumptions are removed and the multi class case is considered immediately. 
First, classification trees will be discussed. When this framework is clear, ensemble methods, that try to improve the trees performance, will be presented and compared. This will be done in the following order: Boosting, Bagging and Random forest. 
%
\subsection{Classification trees}
\label{sub:Classification trees}
The idea behind a classification tree is to make local decisions based on a subset of the predictors. This means that the data is split in subsets, that are again independently split in new subsets and so on. At the end the subsets are given a class label based on some vote-measure between the data points (usually the majority).
\todo{Always majority?}
This makes the method very general, both in terms of nonlinearity and in type of predictors. The method works with continuous, discrete ordered, and categorical predictors. The tree is also easy to interpret, as one can just follow the splits. For prediction one also just follow the tree down to the decision node, giving the predicted class. An example of such a tree can be found in Figure~\ref{fig:cartTree1}.
There are several different algorithms using this framework. They differ in the methods used for splitting, and in how large to grow the tree. Obviously, growing a full tree, i.e. only one training point in each end node, will cause overfitting, while growing it too short will cause underfitting. This bias-variance tradeoff is important to consider, and there are different methods derived to optimize it. More on the bias-variance tradeoff in appendix~\ref{sub:EPE}. In this \colorbox{yellow}{paper?} only one of the classification tree algorithms will be covered. That is one of the most famous tree methods, proposed by \colorbox{yellow}{reference} \cite{breiman} in 1984, and it is called \textit{Classification And Regression Trees}, or \textit{CART}.
%
\subsubsection{CART}
\label{subsub:CART}
<<cartSim, echo=FALSE>>=
# Simple simulation to get nice areas and tree
source("../code/cartSim.R")
@
\colorbox{yellow}{This is a special method, not just a general framework}\\
\url{http://stackoverflow.com/questions/9979461/different-decision-tree-algorithms-with-comparison-of-complexity-or-performance}\\
\colorbox{yellow}{In this section use both \cite{bishop} , \cite{modstat} and \cite{breiman}.}\\
\url{http://edoc.hu-berlin.de/master/timofeev-roman-2004-12-20/PDF/timofeev.pdf}\\
%
The Classificatoin And Regression Trees algorithm only split on one variable at the time. This means that the domain is split into rectangles, aligned withe the different axes. 
Also, each split divide the domain in two parts. This is called binary splitting. This is the easiest way to do this splitting, but as will become clear later, it is pretty effective \todo{Show this}. CART is used both for classification and regression, but only its abilities as a classifier will be considered.
In Figure~\ref{fig:cart} a toy example was simulated to illustrate how the CART algorithm works. The three is made by the r function \verb+rpart+ \cite{rpart}. To make it easy to visualize, only two predictors were used. In Figure~\ref{fig:cartAreas1} the domains created by the splits are displayed, and it shows how it follows the axes. It is clear that the data is not linear, and the figure shows how easy it handles that.
For more than two predictors it is no longer possible to visualize the splits on the actual domain, so a tree view is used instead.
 In Figure~\ref{fig:cartTree1} the tree is displayed. It is clear that it generalizes to more than two dimensions. \\
%
\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{./figures/cartAreas1.pdf}
    \caption{Decisions displayed as boxes.}
    \label{fig:cartAreas1}
  \end{subfigure}%
  \quad
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{./figures/cartTree1.pdf}
    \caption{Decisions displayed as a tree.}
    \label{fig:cartTree1}
  \end{subfigure}
          %(or a blank line to force the subfigure onto a new line)
  \vspace{1\baselineskip}
  \caption{CART run on simulated data in two dimensions. }
  \label{fig:cart}
\end{figure}
%
\\
By now the intuition behind CART should be clear. What is not clear yet is how to grow the tree and how large the tree should be grown. As it is too computationally extensive to create an optimal tree, greedy algorithms for splitting are used for local splitting. A split needs to to be based on a criterion and one of the more used is the \textit{Gini index}
\begin{align}
  Q_m(T) &= \sum^{K}_{k=1} \hat{p}_{mk} (1 - \hat{p}_{mk}),  \\ 
  \label{eq:pmk} 
  \text{where}& \quad \hat{p}_{mk} = \frac{1}{N_m} \sum_{\mathbf{x}_i \in R_m} I\{y_i = k\}.
\end{align}
Here $T$ is the tree, $m$ is a node, $N_m$ is the number of data points in node $m$, $y_i$ is the class of point $i$ and $R_m$ is the region defined by the node.
$\hat{p}_{mk}$ is therefore the proportions of class $k$ in node $m$.
The Gini index gives a measure of \textit{node impurity}, as it increase with the diversity in the node. For nodes with only one class it is zero, and for homogeneous nodes it gets its maximum value. The greedy algorithm used by CART finds the split that gives the lowest total node impurity, and weight the nodes by the probability of sending a new point to that node.  \todo{confirm this} 
\\ \colorbox{yellow}{Comment how the length of a branch in Figure~\ref{fig:cartTree1} is proportional to the quality of the split.} \\ 
Consider the case where a split is performed on node $P$. By splitting on the variable $x_j$, let $R_L(j,s) = \{\mathbf{x} | x_j \leq s\}$ denote the region of the "left" split,  and $R_R(j,s) = \{\mathbf{x} | x_j > s\}$ denote the "right". To find the variable $x_j$ and split point $s$ that gives the lowest node impurity, one solve
\begin{align}
  \min_{j,s} \left\{ \frac{N_L}{N_P} Q_L(T)
  + \frac{N_R}{N_P} Q_R(T) \right\}.
\end{align}
Here $j$ and $s$ lies in $\hat{p}_{mk}$ in \eqref{eq:pmk}.
\todo{interpret gini indiex}
There are other measure of node impurity, like \textit{deviance} and \textit{misclassification error} used by CART. \colorbox{yellow}{These will be visited later}.\\
\colorbox{yellow}{Make sure that Gini is the original choice. If not, comment} \\
\\
It is now clear how to grow a tree, so the next step is to examine the tree size, to prevent over- or under fitting. The immediate idea one might get is to stop when the total Gini index change less than some threshold. This does not take into account that a seemingly worthless split might cause an excellent successive split. 
What CART does instead is to grow a large tree, and then \textit{prune} it back to find a local optimum. To prune a tree, is to collapse any number of internal nodes. 

Let $T_0$ denote the full tree and $T$ a subtree of $T_0$ that can be obtained by pruning. $|T|$ is the number of terminal nodes and $R_{\tau}$ is the region of of the terminal node $\tau$. The goal is to find the subtree $T_\alpha$ that minimizes some cost function $C_\alpha (T)$. It is usually the total terminal node impurity, but regularized by penalizing on the number of terminal nodes
\begin{align}
  C_\alpha (T) = \sum_{\tau = 1}^{|T|} Q_\tau (T) + \alpha |T|. 
\end{align}
This method is referred to as \textit{const-complexity pruning}.
Here the total Gini index can be used for $Q_\tau (T)$, but a more common choice is to use the \textit{misclassification rate} \todo{reference}
\begin{align}
  Q_\tau (T) =  \frac{1}{N_{\tau}} \sum_{\mathbf{x}_i \in R_{\tau}} I\{y_i \neq k(\tau)\},
\end{align}
where $k(\tau)$ is the predicted class in node $\tau$ (majority vote).
\\ \colorbox{yellow}{Is $y_i$ training data, test data, or CV?} \\
However, this is not an optimization problem easily solved by general solvers. Breiman et al. \cite{breiman} suggested a method called \textit{weakest link} pruning to find $T_\alpha$. They sequentially collapse the nodes that gives the smallest increase in $C_\alpha(T)$.
They proved that if continuing to collapse all the way back to a single node, $T_\alpha$ has to be in this sequence of subtrees.  \todo{Need to prove this?}. The only thing remaining then is to find the best $\alpha$. This is usually done by cross-validation, or, if the dataset is large, by misclassification error on a subset the training data, not used to build the tree. The latter is less computationally expensive, but can only be used on larger datasets. The final tree chosen is either the tree with the lowest MCE or the most parsimonious tree within 1 SD of that.
\\ \colorbox{yellow}{Not sure if $T_\alpha$ is found by $\min C_\alpha$ or CV over all sequential trees. Sources don't agree!}
\\ Pruning: \url{http://www.cbcb.umd.edu/~salzberg/docs/murthy_thesis/survey/node14.html}\\
\colorbox{yellow}{9.2.4 modstat Other issues: Categorical predictors} \\ \\
%
\colorbox{yellow}{Somethig on splitting on linear combinations of predictros. Good for prediction bad for interpreabillity}\\
\colorbox{yellow}{Do some simulations?}\\
\\\colorbox{yellow}{Mention other tree building algorithms? Shortly?}

\section{Boosting}
\label{sec:Boosting}
%%fakesubsection Boosting
In this part boosting will be discussed, first in general terms with \textit{Adaboost} as an introductory example. However, the main focus will be on boosting trees and their performance, hereunder mainly \textit{Gradient boosting}. \\
\\
The idea behind boosting is to combine many ''weak'' classifiers to create one ''strong''. This is usually referred to as a committee-based classifier, and bears some resemblance to other committee-based approaches such as \textit{bagging} as will be discussed in section \todo{section}. Roughly said, the algorithms trains weak classifiers sequentially, and for each new classifier the data points are weighted by how hard they were for previous classifiers to classify correctly. At the end all the classifiers are combined and weighted based on how well they preform. The idea is better illustrated in Freund and Schapire's (1997) \todo{ref} algorithm \textit{AdaBoost.M1} \cite{adaboostM1}.
\subsection{Adaboost}
\label{sub:Adaboost}
\colorbox{yellow}{Wikipedia calls adaboost the best out-of-the-box classifier}
\todo{Adaboost or AdaBoost}
\colorbox{yellow}{Any point talking about exponential loss if not adaboost?}\\
\colorbox{yellow}{In R: adabag.}\\
Adaboost.M1 \url{http://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf} This is mentioned in the start of boosting \cite{modstat} \\
Algorithm 10.1 in \cite{modstat} only works for two classes, as $\alpha$ changes sign for $err > 0.5$. \\
\\
Adaboost.M1 was created in (1997) \todo{ref} and \colorbox{yellow}{some history}. Hastie et al. (2006) \cite{modstat} described it as the ''the most popular boosting algorithm''. Only the two class case will be discussed here, but it generalizes easily. The method requires, both for two class and multi class, that the weak classifiers has prediction error less than $1/2$.

Consider output variable coded as $Y \in \{-1, 1\}$. A weak classifier $C(X)$ is chosen to do the ground work. This can for instance be a shallow tree, or some other algorithm that does not perform much better than random guessing. All the data points $\mathbf{x}_i$, $i = 1, 2, \ldots, N$, are initialized with weights $w_i = 1/N$. Then, for $m = 1$, $C_m(\mathbf{x}_i)$ is trained using the weights. $err_m$ is used as a performance measure for $C_m(\mathbf{x}_i)$.
\begin{align}
  err_m =  \frac{\sum^{N}_{i=1} w_i I\{y_i \neq C_m(\mathbf{x}_i)\}}{\sum^{N}_{i=1} w_i}.
\end{align}
Then the weights are updated using 
\begin{align}
  w_i &\leftarrow w_i \exp\left( \alpha_m I\{y_i \neq C_m(\mathbf{x}_i)\} \right), \\
  &\text{where} \quad \alpha_m = \log\left(  \frac{1-err_m}{err_m} \right).
\end{align}
This is repeated for $m = 1, \ldots, M$. Finally the classifier is created
\begin{align}
  C( \mathbf{x}) = \text{sign}\left( \sum^{M}_{m=1} \alpha_m C_m(\mathbf{x}) \right).
\end{align}
There are many choices of weak classifiers, but one of the more common is to use classification trees. It is then important to not grow an optimal tree as with CART, but instead grow a shallow tree or just stumps (one split).
\\ \colorbox{yellow}{Why does the weak classifier has to be weak? Why not grow a large tree?}. An example of this implementation can be found in the r package \verb+adabag+ \cite{adabag}. How to best grow a weak classification tree for AdaBoost will not be further covered in this paper.

\subsection{Forward stagewise Additive Modeling}
\label{sub:Forward stagewise additive modeling}
A common way to view the goal of classification problems is through the expected prediction error (Appendix~\ref{sub:EPE}). The goal is to find
\begin{align}
  \label{eq:fargmin} 
  f^* = \argmin_f \E_{Y, X}[L(Y,F(X))] = \argmin_f \E_Y [L(Y,f(X))|X] ,
\end{align}
where $L(Y, f(X))$ is some loss function. Note that $L$ does not necessarily need to be the misclassification rate, but can be a continuous loss like squared-error loss, $(Y - f(X))^2$. As $f^*$ is usually hard to obtain, a common procedure is restrict $f$ to be a member of a parameterized class of function. In this section $f$ will be an additive expansion in a set of elementary basis functions 
\begin{align}
  \label{eq:additive} 
  f(\mathbf{x}) =  \sum^{M}_{m=1} \beta_m h(\mathbf{x}; \bm{\gamma}_m).
\end{align}
Here $h$ is usually a simple function of $\mathbf{x}$ an parameters $\bm{\gamma}$. These expansions are used in many different learning techniques like neural networks, wavelets, MARS and support vector machines.

Under \eqref{eq:additive}, the solution of \eqref{eq:fargmin} is usually still hard to obtain. Therefor \textit{forward stagewise additive modelling} approximates the solution by sequentially adding new basis functions without changing parameters of basis functions already fitted. The algorithm works the following way:\\
\\
$f_0$ is initialized to 0. For $m = 1, \ldots, M$, $\beta_m$ and $\bm \gamma_m$ is found by
\begin{align}
  \label{eq:forStageWise} 
  (\beta_m, \bm \gamma_m) = \argmin_{\beta, \bm \gamma} \sum^{N}_{i=1} L(y_i, f_{m-1}(\mathbf{x}_i) + \beta h(\mathbf{x}_i; \bm \gamma)).
\end{align}
Then $f_m(\mathbf{x})$ is updated to $f_m(\mathbf{x}) = f_{m-1}(\mathbf{x}) + \beta_m h(\mathbf{x}; \bm \gamma_m)$.\\
\\
If the basis functions are set to be a classifier $h(\mathbf{x}; \bm \gamma) \in \{-1, 1\}$, and the loss is exponential
\begin{align}
  \label{eq:expLoss} 
  L(y, f(\mathbf{x})) = \exp (-y f(\mathbf{x})),
\end{align}
it is pretty straight forward to show \cite{modstat} that forward stagewise modeling is equivalent to the Adaboost.M1 classifier in the previous section. This was not the original motivation for Adaboost.M1. The way Adaboost.M1 fits in the forward stagewise framework, was only discovered years later. However, the forward stargewise framework give the opportunity to compare the Adaboost.M1 to different loss functions. Under the exponential loss in \eqref{eq:expLoss}, the solution of \eqref{eq:fargmin} is 
\begin{align}
  f^*( \mathbf{x}) = \frac{1}{2} \log \frac{P(Y=1 |X=x)}{P(Y=-1 | X = x)} .
\end{align}
So the classification rule in Adabost.M1 approximates one half of the log-odds. That justifies using its sign as a classification rule.
%
\subsubsection{Loss functions}
\label{sub:Loss functions}
<<lossFunctions, echo=FALSE>>=
# Plot of different loss functions
source("../code/lossFunctions.R")
@
\colorbox{yellow}{Drawing of different loss functions}
The choice of loss function is important in terms of both accuracy and computations. While the use of $0/1$ loss might be intuitive, a differentiable loss function might have a computational advantage. By using squared-error loss, $L = (Y - f(X))^2$, \eqref{eq:forStageWise} fit a basis function to the residual of the previous function. There exist fast algorithms for solving these type of problems, but squared-error loss is still not considered a good choice for classification. That can be explained by Figure~\ref{fig:lossFunctions}. The figure shows the loss $L(y, f(\mathbf{x}))$ for different loss functions, where $y \in \left\{ -1, 1 \right\}$. If the classification rule is $C(\mathbf{x}) = \mathrm{sign}(f(\mathbf{x}))$, then a positive margin $y f(\mathbf{x}) > 0$ implies a correct classification, while a negative margin implies a wrong. It is clear from the figure that squared-error loss start increasing when the margin is higher than $1$. As any loss function should penalize a negative margin more than a positive one, this makes squared-error loss not very well suited for classification.

From the figure it is clear that exponential and misclassification loss decrease with the margin and are therefore better choices than square-error loss. But as mentioned earlier, and will become more clear in \colorbox{yellow}{the next section}, a differentiable loss is easier to use. The last loss function in Figure~\ref{fig:lossFunctions} is the \textit{negative binomial log-likelihood}, or \textit{binomial deviance},
\begin{align}
  L(y, f(\mathbf{x})) = \log \left( 1 + \exp (-2 y f(\mathbf{x})) \right).
\end{align}
This locks very similar to the exponential loss, but might be the preferred choice. The main difference is that for increasingly negative margins the exponential loss penalize exponentially, while the binomial deviance penalize linearly. Therefore the binomial deviance is not as vulnerable in noisy settings. The binomial deviance is a very common choice and is was will be used for the rest of this section. For multi class cases, the binomial deviance generalizes to the \textit{multinomial deviance}
\begin{align}
  &L(y, f( \mathbf{x })) = - \sum^{K}_{k=1} I\{y = k\} \log p_k(\mathbf{x}), \\
  &\text{where} \quad p_k(\mathbf{x}) = \frac{\exp (f_k(\mathbf{x}))}{\sum^{K}_{k=1} \exp (f_l(\mathbf{x}))}.
\end{align}
As with logistic regression, this is a probabilistic classifier, ie. it returns class probabilities. That is very useful in fields like data mining, but can also be useful in pure classification when combining into ensembles. \colorbox{yellow}{Need to mention ensemble learning somewhere.}
\colorbox{yellow}{More loss functions?}
%
\begin{figure}[h!]
\begin{center}
    \includegraphics[scale=0.5]{./figures/lossFunctions.pdf}
\end{center}
\caption{Different loss functions $L(y, f)$ as function of the margin $yf$, scaled so all goes through point $(0, 1)$. $y \in \{-1, 1\}$.}
\label{fig:lossFunctions}
\end{figure}
%
\subsection{Gradient boosting}
\label{sub:Gradient boosting}
\colorbox{yellow}{Gradient boosting use regression trees, NOT classification. Include section on regression trees?} \\
\colorbox{yellow}{Both stochastic and "non-stochastic" gradient boosting?} In that case do regularization in \cite{modstat}.
\\ Gradient boosting paper \cite{friedman}. \colorbox{yellow}{Algo 6 is equal to algo 5  for K = 2}\\
\colorbox{yellow}{Take a look at 7. Tree boosting \cite[p.~22]{friedman}} \\
Zhu et al. (2005) generalize exponential loss for K-class problems \cite[p.~349]{modstat}.

\colorbox{yellow}{Look at influence trimmin \cite{friedman}}.
\\
\\Gradient boosting is a smart method developed by Jerome H. Friedman (1999) \cite{friedman}. It is built on the stagewise fashion for arbitrary differentiable loss functions. The idea behind the method is to solve for $f^*$ \eqref{eq:fargmin} numerically, using steepest decent in function space.

Let $g_m$ denote the gradient at step $m$
\begin{align}
  g_{m}(\mathbf{x}) = \left[ \frac{\partial \E_{Y} \left[ L(Y, f(\mathbf{x})) \right | \mathbf{x}]}{\partial f(\mathbf{x})}  \right]_{f(\mathbf{x}) = f_{m-1}(\mathbf{x})},
\end{align}
or assuming sufficient regularity that integration and differentiation can be interchanged, 
\begin{align}
  g_{m}(\mathbf{x}) = \E_{Y} \left[ \frac{\partial L(Y, f(\mathbf{x})) }{\partial f(\mathbf{x})}  | \mathbf{x}\right]_{f(\mathbf{x}) = f_{m-1}(\mathbf{x})}.
\end{align}
$f_m$ is then updated to
\begin{align}
  f_m(\mathbf{x}) = f_{m-1}(\mathbf{x}) - \rho_m  g_m(\mathbf{x}),
\end{align}
where $\rho_m$ is the step length found by
\begin{align}
  \rho_m = \argmin_{\rho}  \E_{Y} \left[ L(Y, f_{m-1}(\mathbf{x}) - \rho g_m(\mathbf{x})) \right | \mathbf{x}].
\end{align}
By repeating these steps, the function is moving towards a minimum of $L$ (in the steepest decent direction $-g_m$), though in a very greedy matter. \\
\\
In the steepest decent algorithm above above $\mathbf{x}$ is considered a stochastic variable, but that is not really the case. For finite data $(\mathbf{x}_i, y_i)$ for $i = 1, \ldots, N$, $E_Y \left[\: \cdot \:| \mathbf{x} \right]$ can not be accurately estimated by the data values, and is only defined at the training points. As the goal is to find a good approximation for $f^*(\mathbf{x})$ for all $\mathbf{x}$, a different approach is needed. Friedman's approach to tackle this was to impose an additive parameterized form of $f$ as in \eqref{eq:additive}, and do parameter optimization over those constraints. By doing the fitting in a stagewise matter, the optimization problem was reduced to the forward stagewise additive modeling problem \eqref{eq:forStageWise}. As this is often difficult to solve, he approximated a solution based on the steepest decent algorithm.

For a given $f_{m-1}(\mathbf{x})$, the function $\beta_m h(\mathbf{x}; \mathbf{\gamma}_m)$ can be viewed as the best step toward the databased $f^*$, under the given constraint \eqref{eq:additive}, and is can therefore be considered a steepest decent step. It is often hard to find the solution to \eqref{eq:forStageWise}, but as $\beta_m h(\mathbf{x}; \mathbf{\gamma}_m)$ is comparable to the unconstrained negative gradient $g_m$, it can be fitted to $-g_m$ instead. This is done by finding the $\beta_m h(\mathbf{x}; \mathbf{\gamma}_m)$ highest correlated to $-g_m(\mathbf{x})$,
\begin{align}
  \label{eq:gradBoostFit} 
  \bm \gamma_m = \argmin_{\beta, \bm \gamma} \sum^{N}_{i=1} \left( -g_m(\mathbf{x}_i) - \beta h(\mathbf{x}_i; \bm \gamma) \right)^2.
\end{align}
Then $f_m$ is updated using
\begin{align}
  &f_m(\mathbf{x}) = f_{m-1}(\mathbf{x}) + \rho_m h(\mathbf{x}; \mathbf{\gamma}_{m}),\\
  \label{eq:lineSearch} 
  &\text{where} \quad \rho_m = \argmin_\rho \sum^{N}_{i=1} L(y_i, f_{m-1}(\mathbf{x}) + \rho h(\mathbf{x}; \mathbf{\gamma}_m)).
\end{align}
These last three equations are know as the \textit{gradient boosting algorithm}.
As \eqref{eq:forStageWise} is replaced by \eqref{eq:gradBoostFit}, $\tilde{y}_i = -g_m(\mathbf{x}_i)$ is often referred to as the ''pseudo-responce''. 
Note here that $h$ and $f$ are continuous functions and not classifiers. The classification is done after all the $M$ iterations, usually trough some relationship between $f_M$ and class probabilities. For the binomial deviance loss this relationships are the log-odds,
\begin{align}
  &P(Y = 1 |  \mathbf{x}) = \left( 1+ \exp\left( -2 f_M (\mathbf{x}) \right) \right)^{-1},\\
  &P(Y = -1 |  \mathbf{x}) = \left( 1+ \exp\left( 2 f_M (\mathbf{x}) \right) \right)^{-1}.
\end{align}
Later it will be show how powerful this simple algorithm is, but first trees will be used as the basis functions $\beta_m h(\mathbf{x}; \mathbf{\gamma}_m)$.

\subsubsection{Regression Trees}
\label{sub:Regression Trees}
It was earlier mentioned that the basis functions $h(\mathbf{x}; \mathbf{\gamma}_m)$ in gradient boosting are continuous functions. Therefore, classification trees can not be used and the regression framework needs to be explored.
In section \ref{sub:Classification trees} it was show how classification trees can be fitted using the CART method. CART stands for \textit{Classification And Regression Trees} and can obviously also be used for regression. The ides behind fitting regression trees are almost identical to classification, but the functions used during growing, estimating and pruning are different. However, this will not be investigated deeply, only what is needed in the boosting framework.\\
\\
A regression tree $T$ can be represented in the following form 
\begin{align}
  T(\mathbf{x}; \left\{ c_j, R_j \right\}_{j = 1}^J)  = \sum^{J}_{j=1} c_j I\left\{ \mathbf{x} \in R_j \right\}.
\end{align}
As with classification all the regions $R_j$ are disjunct and collectively cover all possible values of $\mathbf{x}$. If fitting a tree to the response $\tilde y$ under squared error loss, it is straight forward to see that the solution in each region $R_j$ is just the region average
\begin{align}
  \hat{c}_j = \mathrm{ave}(\tilde y_i | \mathbf{x}_i \in R_j).
\end{align}
The tree is grown sequentially, similarly as with classification, by finding the split that minimize the total squared error loss. When a larger tree is grown, it can then be pruned back using \textit{const-complexity pruning} based on the squared error and penalized by the number of terminal nodes. In the boosing context however, pruning is not really an alternative. Usually less than 10 end-nodes will be sufficient \cite[p.~363]{modstat}, and thus growing a larger tree and pruning it back will only be very computationally expensive, probably with worse end results. \todo{Discuss this later?}\\
\\
In gradient boosting trees are fitted in \eqref{eq:gradBoostFit}, in place of $\beta h(\mathbf{x}_i; \mathbf{\gamma})$ and $f_m$ can be created by
\begin{align}
  f_m(\mathbf{x}) = f_{m-1}(\mathbf{x}) + \rho_m T(\mathbf{x}; \left\{ c_{m j}, R_{m j} \right\}_{j=1}^{J}),
\end{align}
where $\rho_m$ is the solution of the line search in \eqref{eq:lineSearch}.  However, what is more common is to use one variable $\eta_{m j}$ instead of $\rho_m c_{m j}$. Thus one fit $J$ separate basis functions instead of a single additive. This gives the opportunity to better tune the coefficients and improving the fit. So $f_m$ is set to 
\begin{align}
  f_m(\mathbf{x}) = f_{m-1}(\mathbf{x}) +  \sum^{J}_{j=0} \eta_{m j} I\left\{ \mathbf{x} \in R_{m j} \right\}.
\end{align}
Do to the disjunct nature of the trees, each $\eta_{m j}$ can be found independently.
\begin{align}
  \eta_{m j} = \argmin_\eta  \sum_{\mathbf{x}_i \in R_{m j}} L(y_i, f_{m-1}(\mathbf{x}) + \eta).
\end{align}

\subsubsection{The two-class logistic regression classifier}
\label{sub:The two-class logistic regression classifier}
The framework for gradient boosting is now reviewed, and final step is to summarize the algorithm for the binomial deviance loss. As before $y \in \left\{ -1, 1 \right\}$ and the loss is 
\begin{align}
  &L(Y, f(\mathbf{x})) = \log (1 + \exp (-2 Y f(\mathbf{x}))),\\
  &\text{where} \quad f(\mathbf{x}) = \frac{1}{2} \log \left( \frac{P(Y = 1 | \mathbf{x})}{P(Y = -1 | \mathbf{x})}  \right).
\end{align}
$f_0(\mathbf{x})$ is initialized to $\frac{1}{2} \log \left( \frac{1+\bar{y}}{1-\bar{y}}  \right)$.




\todo{Section on boosting and overfitting}

\subsection{Stochastic gradient boosting}
\label{sub:Stochastic gradient boosting}

\colorbox{yellow}{Section on stochastic gradient boosting}\\
ada: an R package for stochastic boosting \url{http://dept.stat.lsa.umich.edu/~gmichail/ada_final.pdf} \\
Friedman \url{https://statweb.stanford.edu/~jhf/ftp/stobst.pdf} \\
\section{Bagging}
\label{sec:Bagging}

\section{Random forest}
\label{sec:Random forest}

