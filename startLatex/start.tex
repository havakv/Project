% Created: lø. 07. feb. 14:10:23 +0100 2015
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc} % Norwegian letters
\usepackage{amsmath}
\usepackage{float}  % Used for minipage and stuff.
\usepackage{wrapfig} % Wrap text around figure wrapfig [tab]
\usepackage{graphicx}
\usepackage{enumerate} % Use e.g. \begin{enumerate}[a)]
\usepackage[font={small, it}]{caption} % captions on figures and tables
\usepackage[toc,page]{appendix} % Page make Appendices title, toc fix table of content 
\usepackage{todonotes} % Notes. Use \todo{"text"}. Comment out \listoftodos
\usepackage{microtype} % Improves spacing. Include AFTER fonts
\usepackage{hyperref} % Use \autoref{} and \nameref{}
\hypersetup{backref,
  colorlinks=true,
  breaklinks=true,
  %hidelinks, %uncomment to make links black
  linkcolor=blue,
  urlcolor=blue
}
\usepackage[all]{hypcap} % Makes hyperref jup to top of pictures and tables
%
%-------------------------------------------------------------------------------
% Page layout
%\usepackage{showframe} % Uncomment if you want the margin frames
\usepackage{fullpage}
\topmargin=-0.25in
%\evensidemargin=-0.3in
%\oddsidemargin=-0.3in
%\textwidth=6.9in
%\textheight=9.5in
\headsep=0.25in
\footskip=0.50in

%-------------------------------------------------------------------------------
% Header and footer
\usepackage{lastpage} % To be able to add last page in footer.
\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancy} % Use "fancyplain" for header in all pages
%\renewcommand{\chaptermark}[1]{ \markboth{#1}{} } % Usefull for book?
\renewcommand{\sectionmark}[1]{ \markright{\thesection\ #1}{} } % Remove formating and nr.
%\fancyhead[LE, RO]{\footnotesize\leftmark}
%\fancyhead[RO, LE]{\footnotesize\rightmark}
\lhead[]{\AuthorName}
\rhead[]{\rightmark}
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{Page\ \thepage\ of\ \protect\pageref*{LastPage}} % Page numbering for right footer
\renewcommand{\headrulewidth}{1pt} % header underlines
\renewcommand{\footrulewidth}{1pt} % footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

%-------------------------------------------------------------------------------
% Suppose to make it easier for LaTeX to place figures and tables where I want.
\setlength{\abovecaptionskip}{0pt plus 1pt minus 2pt} % Makes caption come closer to figure.
%\setcounter{totalnumber}{5}
%\renewcommand{\textfraction}{0.05}
%\renewcommand{\topfraction}{0.95}
%\renewcommand{\bottomfraction}{0.95}
%\renewcommand{\floatpagefraction}{0.35}
%
% Math short cuts for expectation, variance and covariance
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
% Commands for argmin and argmax
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-----------------------------------------------------------------------------
%	TITLE SECTION
%-----------------------------------------------------------------------------
\newcommand{\AuthorName}{Håvard Kvamme} % Your name

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{NTNU} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Project on classification \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{\AuthorName } % Your name

\date{\normalsize\today} % Today's date or a custom date
\begin{document}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listoftodos{}
%
\section{Questions}
\label{sec:Questions...}
\begin{itemize}
  \item Discuss the historical significance of the different methods?
  \item Give methods for both large and small amounts of data?
  \item Usually use the most parsimonious model within one sd? Can it be proved that this is a good idea? Is it actually used?
\end{itemize}
%
\section{Intro}
\label{sec:Intro}
\begin{verbatim}
  
Want to look at:
Framework
- Cross validation 
- EPE (Expected prediction error)
- Bias-variance tradeoff 
- Deviance and cross-entropy error (the same???)
- Regularization (logistic, anyone else?) L1 and L2.
- Bayes Classifier (learn to plot this to see how well our methods perform).
- Bootstrap
- NOT Kernel?
- Ordered VS Unordered classes?
- Talk about standardizing data (0,1) before using it?
- Section about how to compare perfromance of different classifiers.
     When a method is significantly better.
- Statistical decision theory (Bishop)

Methods
- Logistic regression (local, nonparametric and gams?) (bayesian?) (mixture of logistic models (Bishop)
   Should i talk about optimization algorithms? To what extent?
- LDA (QDA, RDA) (Fisher's linear ldiscriminant?)
- Trees (CART, bagging, RF, Boosting) 
- Neural Nets (and deep nural nets)
- Combining models (Bishop ch 14): Boosing, hierarchical mixture of exterts...

Other methods?
- k-NN
- Naive Bayes (usefull for highdimm methods?)
- SVM's
- Projection pursuit regression (to introduce neural networks?)
\end{verbatim}
%
%

%
\subsection{Statistical decision theory}
\label{sub:Statistical desicion theory}
\colorbox{yellow}{See mod.stat book!} \\
Here you can find both EPE and Bayes Classifier!

\section{Linear classifiers}
\label{sec:Linear classifiers}
Fist one will consider linear methods for classification. Two of the most famous classifiers are LDA and Logistic Regression.
\begin{verbatim}
  Other Algorithms:
  -The perseptron algorithm (linear discriminant for pattern recognition)

\end{verbatim}
\subsection{LDA}
\label{sub:LDA}
LDA, or linear discriminant analysis is \ldots \\
Commonly used as dimensionality reduction before later classification \url{http://en.wikipedia.org/wiki/Linear_discriminant_analysis#Fisher.27s_linear_discriminant}.\\
Based on Gaussian, but perform well on non-Gaussian sets. Se discussion in mod.stat (both exercise about L2) and about simple (linear/quadratic) boundaries.\\
\colorbox{yellow}{Se end of mod.stat 4.3.1, on other regularization than RDA.}\\
Fisher's linear discriminant. See Bishop, wikipedia and mod.stat. Might be a good introduction to LDA? \\
Multiclass LDA (see wikipedia for easy generalization). \\
Not robust to gross outliers (see p 128 mod.stat).
%
\subsection{Logistic Regression}
\label{sub:Logistic Regression}
\colorbox{yellow}{Local, nonparametric, gams?}\\
Local, or should I stay away from everything that has to do with kernels? \\
Use glm-notes! \\
It is used alot for data analysis and inference tool, where the goal is to understand the role of the input variables in explaining the outcome (mod.stat).\\
Add a section about to which extent it is used today? \\
Add penalty functions L1 and L2? \\
L1: \verb+glampath+ p. 126 mod.stat.\\
Should I do something og Bayesian logistic regression?
%
\subsubsection{LDA vs. Logistic}
\label{sub:LDA vs. Logistic}
Se section about it in mod.stat. \\
Do simulations to confirm the differences. 

\section{Tree-based methods}
\label{sec:Tree-based methods}


\subsection{CART}
\label{sub:CART}
Look at C5.0? (mod.stat). \\
Linear combination splits p. 312 mod.stat. \\
hierarchical mixtures of experts (mod.stat and Bishop) \\ 
Talk about Prim? (Not a binary tree).\\ 
Talk about Mars? It can be used for classification! \\

\subsection{Bagging}
\label{sub:Bagging}
Bootstrap aggregation (bagging) is a way to reduce variance of variance of more or less unbiased methods. Works well on trees. \\
\colorbox{yellow}{Make a section about Bootstraping} \\
Under classification with 0-1 loss, bagging will make a good classifier better and a bad classifier worse (mod.stat). \\
Talk about how boosting take care of some problems with bagging (mod.stat).

\subsection{Random Forest}
\label{sec:Random Forest}
Mention of how to compare methods (when a method is significantly better) p 590 (mod.stat). \\
OOB, makes it possible to fit and cross-validate simultaneously. \\
Variable importance.

\section{Boosting}
\label{sec:Boosting}


\section{Neural Networks}
\label{sec:Neural Networks}
\url{https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/}
\subsection{Deep learning}
\label{sub:Deep learning}
\url{http://www.quora.com/What-are-the-best-packages-for-deep-learning-in-R}\\
\url{http://www.quora.com/search?q=deep+learning}\\
\url{http://www.quora.com/What-are-some-fundamental-deep-learning-papers-for-which-code-and-data-is-available-to-reproduce-the-result-and-on-the-way-grasp-deep-learning}
\url{http://www.quora.com/Is-there-a-better-tutorial-for-deep-learning}












%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\clearpage
%\begin{appendices}
  %% Use \section
  %% Can use \phantomsection in text to get back
%\end{appendices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{thebibliography}{99} % At most 99 references.
%% Use "bibit" to generate bibitem
%\end{thebibliography}
%
\end{document}
